{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NZ1cPpAtS_R"
   },
   "source": [
    "![LiU_logo.png](data:image/png;base64,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)\n",
    "## Linkoping University: TDDC17 Artificial Intelligence\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSKKEe1vtl2_"
   },
   "source": [
    "**Lab5 : Deep Learning**\n",
    "\n",
    "**Lab5.1 : Understading the basic of Pytorch and Tensorflow framework**\n",
    "\n",
    "  In the field of deep learning, PyTorch and TensorFlow are two of the most popular frameworks. While both are used for building and training neural networks, they have different histories and design philosophies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I-wbE63zOeH"
   },
   "source": [
    "**Relationship Between PyTorch and TensorFlow**\n",
    "\n",
    "Despite being competitors, PyTorch and TensorFlow share many similarities and have influenced each other. Here are some key points of their relationship:\n",
    "\n",
    "* Design Philosophy: PyTorch focuses on flexibility and dynamic computation graphs, making it suitable for research and prototyping. In contrast, TensorFlow emphasizes performance and static computation graphs, which are ideal for production environments. TensorFlow 2.0 introduced Eager Execution mode, which partially draws from PyTorch's design philosophy.\n",
    "\n",
    "* Community and Ecosystem: Both frameworks boast large communities and rich ecosystems, offering numerous models and tools. TensorFlow's TensorFlow Hub and PyTorch's PyTorch Hub are excellent platforms for sharing pre-trained models.\n",
    "\n",
    "* Interoperability: Both frameworks are continually improving their interoperability. For instance, the ONNX (Open Neural Network Exchange) format allows model conversion between PyTorch and TensorFlow, enhancing compatibility and collaboration between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYBnz_DhFLCl"
   },
   "source": [
    "**I. Pytorch and Tensorflow Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee-nZMDqtCBl"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPIeN0yJG1CK"
   },
   "source": [
    "# I.1 Pytorch Hand-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "llxEVMREG88J"
   },
   "outputs": [],
   "source": [
    "## import torch package\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzsJxfVBHIiD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a tensor of size 2x3x4\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LbK_6ImcHwk6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the size of the tensor\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6C-dGqwKH0xU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point in a 24 dimensional space\n",
      "organised in 3 sub-dimensions\n"
     ]
    }
   ],
   "source": [
    "# prints dimensional space and sub-dimensions\n",
    "print(f'point in a {t.numel()} dimensional space')\n",
    "print(f'organised in {t.dim()} sub-dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n1djyiCMII63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This resizes the tensor permanently\n",
    "r = torch.Tensor(t)\n",
    "r.resize_(3, 8)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zxOL3M4wIeA3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a 1D tensor of integers 1 to 4\n",
    "v = torch.Tensor([1, 2, 3, 4])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l8Upm2oVItFN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 1, size: 4\n"
     ]
    }
   ],
   "source": [
    "# Print number of dimensions (1D) and size of tensor\n",
    "print(f'dim: {v.dim()}, size: {v.size()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-aJopdypIv6P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 2., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.Tensor([1, 0, 2, 0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "76CGG3xaIzj9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 6., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "v * w\n",
    "# 1, 0, 6, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kePuuavbI5tS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar product: 1*1 + 2*0 + 3*2 + 4*0\n",
    "v @ w\n",
    "# 1, 0, 6, 0 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ntSIfDgoJArg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  4.,  9., 16.]) tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "# Square all elements in the tensor\n",
    "print(v.pow(2), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nwOaqL1qJO05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2x4 tensor(matrix 2D)\n",
    "m = torch.Tensor([[2, 5, 3, 7],\n",
    "                  [4, 2, 1, 9]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RhDvnwV5JXEa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hwXGqBJwJdBI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "m[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YZbYw2iWJgrO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "m[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "op3juZ2oKCva"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.7000,  2.7000, -0.3000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor from 5.7 to -2.1 with each having a space of -3\n",
    "torch.arange(5.7, -2.1, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XmbttCKQKLlZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000, 3.2632, 3.5263, 3.7895, 4.0526, 4.3158, 4.5789, 4.8421, 5.1053,\n",
       "         5.3684, 5.6316, 5.8947, 6.1579, 6.4211, 6.6842, 6.9474, 7.2105, 7.4737,\n",
       "         7.7368, 8.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns a 1D tensor of steps equally spaced points between start=3, end=8 and steps=20\n",
    "torch.linspace(3, 8, 20).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r5db5M3uKOYa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with 0's\n",
    "torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wQzh3l27KRa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with 1's\n",
    "torch.ones(3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dY2ePI3HK1f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move your tensor to GPU device 0 if there is one (first GPU in the system)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "m.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqkbIYkIKcVu"
   },
   "source": [
    "# I.1 Tensorflow Hand-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ugdHTPyDLhfV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "d6Iv0C4zLqxd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow constant\n",
    "tensor_constant = tf.constant([[1, 2], [3, 4]])\n",
    "print(tensor_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "zGOop-qfL9fh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[5, 6],\n",
      "       [7, 8]], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow variable\n",
    "tensor_variable = tf.Variable([[5, 6], [7, 8]])\n",
    "print(tensor_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "E6rpJc5xMEwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6  8]\n",
      " [10 12]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Perform element-wise addition\n",
    "tensor_sum = tf.add(tensor_constant, tensor_variable)\n",
    "print(tensor_sum)\n",
    "# 6, 8, 10, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Z6nB5kCYMIz8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[19 22]\n",
      " [43 50]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Perform matrix multiplication\n",
    "tensor_product = tf.matmul(tensor_constant, tensor_variable)\n",
    "print(tensor_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vAIX8x99MMPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]], shape=(4, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Reshape a tensor\n",
    "tensor_reshaped = tf.reshape(tensor_constant, [4, 1])\n",
    "print(tensor_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "R4gzwdsNMSnp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  4]\n",
      " [ 9 16]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Apply a function to a tensor element-wise\n",
    "tensor_squared = tf.square(tensor_constant)\n",
    "print(tensor_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fHSTuNFfMW_V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a TensorFlow tensor to a NumPy array\n",
    "numpy_array = tensor_constant.numpy()\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07UjMj61NRY9"
   },
   "source": [
    "##II Linear regression with tensorflow and pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YP1ndz2ROmo5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8b2zjLJTSFh"
   },
   "source": [
    "Given a simple linear regression problem\n",
    "\n",
    "x= [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "y= [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]\n",
    "\n",
    "y = 2*x -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blmz0zJdPKtg"
   },
   "source": [
    "# II.1 Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "z8u97phwO6Ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reevalex/Master/Term-1/TDDC17/group/labs/lab3_deep_learning/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# we design our model using the Sequential, which is a linear stack of layers. In this model, we use only one layer(neuron)\n",
    "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "sGniRaaIPcKR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.src.optimizers.sgd.SGD at 0x152c28f20>, 'mean_squared_error')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer and loss functions to train our Neural Network model. In this, we use SGD optimizer and MSE as our loss function\n",
    "# TODO: Implement the model compilation step using the appropriate optimizer and loss function\n",
    "# The optimizer should be 'sgd' and the loss function should be 'mean_squared_error'\n",
    "# Hint: Use the model.compile() method\n",
    "\n",
    "##<YOUR CODE>\n",
    "model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"mean_squared_error\"\n",
    ")\n",
    "\n",
    "model.optimizer, model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "tw-kNpRURVbW"
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MEGmgNxqRbl7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 6.3023\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.1435\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.2280\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.5040\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.9308\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.4762\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.1151\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.8275\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.5979\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.4140\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.2661\n",
      "Epoch 12/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1465\n",
      "Epoch 13/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0494\n",
      "Epoch 14/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.9700\n",
      "Epoch 15/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.9045\n",
      "Epoch 16/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8501\n",
      "Epoch 17/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8044\n",
      "Epoch 18/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7657\n",
      "Epoch 19/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7325\n",
      "Epoch 20/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.7038\n",
      "Epoch 21/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6785\n",
      "Epoch 22/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6561\n",
      "Epoch 23/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6359\n",
      "Epoch 24/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6176\n",
      "Epoch 25/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6008\n",
      "Epoch 26/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5852\n",
      "Epoch 27/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5706\n",
      "Epoch 28/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5569\n",
      "Epoch 29/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5438\n",
      "Epoch 30/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5314\n",
      "Epoch 31/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5195\n",
      "Epoch 32/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5081\n",
      "Epoch 33/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4970\n",
      "Epoch 34/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4863\n",
      "Epoch 35/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4760\n",
      "Epoch 36/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4659\n",
      "Epoch 37/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4561\n",
      "Epoch 38/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4465\n",
      "Epoch 39/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4372\n",
      "Epoch 40/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4281\n",
      "Epoch 41/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4193\n",
      "Epoch 42/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4106\n",
      "Epoch 43/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4021\n",
      "Epoch 44/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3938\n",
      "Epoch 45/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3857\n",
      "Epoch 46/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3777\n",
      "Epoch 47/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3699\n",
      "Epoch 48/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3623\n",
      "Epoch 49/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3549\n",
      "Epoch 50/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3476\n",
      "Epoch 51/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3404\n",
      "Epoch 52/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3334\n",
      "Epoch 53/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3266\n",
      "Epoch 54/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.3198\n",
      "Epoch 55/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3133\n",
      "Epoch 56/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3068\n",
      "Epoch 57/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.3005\n",
      "Epoch 58/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2944\n",
      "Epoch 59/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2883\n",
      "Epoch 60/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2824\n",
      "Epoch 61/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2766\n",
      "Epoch 62/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2709\n",
      "Epoch 63/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2653\n",
      "Epoch 64/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2599\n",
      "Epoch 65/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2546\n",
      "Epoch 66/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2493\n",
      "Epoch 67/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2442\n",
      "Epoch 68/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2392\n",
      "Epoch 69/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2343\n",
      "Epoch 70/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2295\n",
      "Epoch 71/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2247\n",
      "Epoch 72/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2201\n",
      "Epoch 73/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2156\n",
      "Epoch 74/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2112\n",
      "Epoch 75/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2068\n",
      "Epoch 76/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2026\n",
      "Epoch 77/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1984\n",
      "Epoch 78/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1944\n",
      "Epoch 79/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1904\n",
      "Epoch 80/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1865\n",
      "Epoch 81/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1826\n",
      "Epoch 82/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1789\n",
      "Epoch 83/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1752\n",
      "Epoch 84/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1716\n",
      "Epoch 85/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1681\n",
      "Epoch 86/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1646\n",
      "Epoch 87/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1612\n",
      "Epoch 88/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1579\n",
      "Epoch 89/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1547\n",
      "Epoch 90/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1515\n",
      "Epoch 91/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1484\n",
      "Epoch 92/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1453\n",
      "Epoch 93/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1424\n",
      "Epoch 94/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1394\n",
      "Epoch 95/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1366\n",
      "Epoch 96/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1338\n",
      "Epoch 97/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1310\n",
      "Epoch 98/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1283\n",
      "Epoch 99/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1257\n",
      "Epoch 100/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1231\n",
      "Epoch 101/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1206\n",
      "Epoch 102/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1181\n",
      "Epoch 103/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1157\n",
      "Epoch 104/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1133\n",
      "Epoch 105/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1110\n",
      "Epoch 106/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1087\n",
      "Epoch 107/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1065\n",
      "Epoch 108/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1043\n",
      "Epoch 109/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1021\n",
      "Epoch 110/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1000\n",
      "Epoch 111/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0980\n",
      "Epoch 112/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0960\n",
      "Epoch 113/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0940\n",
      "Epoch 114/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0921\n",
      "Epoch 115/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0902\n",
      "Epoch 116/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0883\n",
      "Epoch 117/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0865\n",
      "Epoch 118/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0847\n",
      "Epoch 119/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0830\n",
      "Epoch 120/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0813\n",
      "Epoch 121/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0796\n",
      "Epoch 122/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0780\n",
      "Epoch 123/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0764\n",
      "Epoch 124/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0748\n",
      "Epoch 125/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0733\n",
      "Epoch 126/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0718\n",
      "Epoch 127/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0703\n",
      "Epoch 128/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0689\n",
      "Epoch 129/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0674\n",
      "Epoch 130/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0661\n",
      "Epoch 131/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0647\n",
      "Epoch 132/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0634\n",
      "Epoch 133/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0621\n",
      "Epoch 134/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0608\n",
      "Epoch 135/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0595\n",
      "Epoch 136/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0583\n",
      "Epoch 137/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0571\n",
      "Epoch 138/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0559\n",
      "Epoch 139/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0548\n",
      "Epoch 140/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0537\n",
      "Epoch 141/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0526\n",
      "Epoch 142/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0515\n",
      "Epoch 143/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0504\n",
      "Epoch 144/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0494\n",
      "Epoch 145/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0484\n",
      "Epoch 146/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0474\n",
      "Epoch 147/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0464\n",
      "Epoch 148/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0455\n",
      "Epoch 149/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0445\n",
      "Epoch 150/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0436\n",
      "Epoch 151/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0427\n",
      "Epoch 152/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0418\n",
      "Epoch 153/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0410\n",
      "Epoch 154/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0401\n",
      "Epoch 155/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0393\n",
      "Epoch 156/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0385\n",
      "Epoch 157/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0377\n",
      "Epoch 158/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0369\n",
      "Epoch 159/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0362\n",
      "Epoch 160/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0354\n",
      "Epoch 161/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0347\n",
      "Epoch 162/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0340\n",
      "Epoch 163/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0333\n",
      "Epoch 164/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0326\n",
      "Epoch 165/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0319\n",
      "Epoch 166/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0313\n",
      "Epoch 167/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0306\n",
      "Epoch 168/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0300\n",
      "Epoch 169/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0294\n",
      "Epoch 170/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0288\n",
      "Epoch 171/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0282\n",
      "Epoch 172/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0276\n",
      "Epoch 173/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0271\n",
      "Epoch 174/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0265\n",
      "Epoch 175/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0260\n",
      "Epoch 176/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0254\n",
      "Epoch 177/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0249\n",
      "Epoch 178/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0244\n",
      "Epoch 179/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0239\n",
      "Epoch 180/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0234\n",
      "Epoch 181/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0229\n",
      "Epoch 182/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0224\n",
      "Epoch 183/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0220\n",
      "Epoch 184/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0215\n",
      "Epoch 185/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0211\n",
      "Epoch 186/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0207\n",
      "Epoch 187/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0202\n",
      "Epoch 188/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0198\n",
      "Epoch 189/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0194\n",
      "Epoch 190/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0190\n",
      "Epoch 191/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0186\n",
      "Epoch 192/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0182\n",
      "Epoch 193/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0179\n",
      "Epoch 194/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0175\n",
      "Epoch 195/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0171\n",
      "Epoch 196/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0168\n",
      "Epoch 197/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0164\n",
      "Epoch 198/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0161\n",
      "Epoch 199/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0158\n",
      "Epoch 200/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0155\n",
      "Epoch 201/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0151\n",
      "Epoch 202/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0148\n",
      "Epoch 203/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0145\n",
      "Epoch 204/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0142\n",
      "Epoch 205/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0139\n",
      "Epoch 206/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0136\n",
      "Epoch 207/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0134\n",
      "Epoch 208/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0131\n",
      "Epoch 209/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0128\n",
      "Epoch 210/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0126\n",
      "Epoch 211/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0123\n",
      "Epoch 212/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0120\n",
      "Epoch 213/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0118\n",
      "Epoch 214/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0116\n",
      "Epoch 215/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0113\n",
      "Epoch 216/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0111\n",
      "Epoch 217/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0109\n",
      "Epoch 218/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0106\n",
      "Epoch 219/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0104\n",
      "Epoch 220/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0102\n",
      "Epoch 221/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0100\n",
      "Epoch 222/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0098\n",
      "Epoch 223/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0096\n",
      "Epoch 224/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0094\n",
      "Epoch 225/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0092\n",
      "Epoch 226/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0090\n",
      "Epoch 227/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0088\n",
      "Epoch 228/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0086\n",
      "Epoch 229/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0085\n",
      "Epoch 230/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0083\n",
      "Epoch 231/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0081\n",
      "Epoch 232/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080\n",
      "Epoch 233/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0078\n",
      "Epoch 234/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0076\n",
      "Epoch 235/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0075\n",
      "Epoch 236/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0073\n",
      "Epoch 237/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0072\n",
      "Epoch 238/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0070\n",
      "Epoch 239/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0069\n",
      "Epoch 240/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0067\n",
      "Epoch 241/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0066\n",
      "Epoch 242/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0065\n",
      "Epoch 243/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0063\n",
      "Epoch 244/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0062\n",
      "Epoch 245/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0061\n",
      "Epoch 246/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0059\n",
      "Epoch 247/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0058\n",
      "Epoch 248/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0057\n",
      "Epoch 249/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0056\n",
      "Epoch 250/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0055\n",
      "Epoch 251/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0054\n",
      "Epoch 252/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0053\n",
      "Epoch 253/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051\n",
      "Epoch 254/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0050\n",
      "Epoch 255/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0049\n",
      "Epoch 256/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0048\n",
      "Epoch 257/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0047\n",
      "Epoch 258/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0046\n",
      "Epoch 259/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045\n",
      "Epoch 260/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0044\n",
      "Epoch 261/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044\n",
      "Epoch 262/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0043\n",
      "Epoch 263/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042\n",
      "Epoch 264/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041\n",
      "Epoch 265/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040\n",
      "Epoch 266/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0039\n",
      "Epoch 267/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038\n",
      "Epoch 268/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038\n",
      "Epoch 269/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0037\n",
      "Epoch 270/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036\n",
      "Epoch 271/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035\n",
      "Epoch 272/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035\n",
      "Epoch 273/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0034\n",
      "Epoch 274/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033\n",
      "Epoch 275/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0033\n",
      "Epoch 276/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0032\n",
      "Epoch 277/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031\n",
      "Epoch 278/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031\n",
      "Epoch 279/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030\n",
      "Epoch 280/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0029\n",
      "Epoch 281/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0029\n",
      "Epoch 282/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028\n",
      "Epoch 283/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028\n",
      "Epoch 284/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027\n",
      "Epoch 285/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026\n",
      "Epoch 286/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026\n",
      "Epoch 287/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025\n",
      "Epoch 288/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025\n",
      "Epoch 289/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024\n",
      "Epoch 290/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024\n",
      "Epoch 291/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0023\n",
      "Epoch 292/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0023\n",
      "Epoch 293/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022\n",
      "Epoch 294/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022\n",
      "Epoch 295/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022\n",
      "Epoch 296/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021\n",
      "Epoch 297/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0021\n",
      "Epoch 298/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020\n",
      "Epoch 299/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020\n",
      "Epoch 300/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019\n",
      "Epoch 301/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0019\n",
      "Epoch 302/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019\n",
      "Epoch 303/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018\n",
      "Epoch 304/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018\n",
      "Epoch 305/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017\n",
      "Epoch 306/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017\n",
      "Epoch 307/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0017\n",
      "Epoch 308/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016\n",
      "Epoch 309/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016\n",
      "Epoch 310/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016\n",
      "Epoch 311/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0015\n",
      "Epoch 312/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0015\n",
      "Epoch 313/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015\n",
      "Epoch 314/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015\n",
      "Epoch 315/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0014\n",
      "Epoch 316/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014\n",
      "Epoch 317/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014\n",
      "Epoch 318/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013\n",
      "Epoch 319/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0013\n",
      "Epoch 320/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0013\n",
      "Epoch 321/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0013\n",
      "Epoch 322/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012\n",
      "Epoch 323/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012\n",
      "Epoch 324/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0012\n",
      "Epoch 325/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012\n",
      "Epoch 326/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011\n",
      "Epoch 327/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011\n",
      "Epoch 328/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011\n",
      "Epoch 329/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011\n",
      "Epoch 330/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0010\n",
      "Epoch 331/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010\n",
      "Epoch 332/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.9806e-04\n",
      "Epoch 333/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.7756e-04\n",
      "Epoch 334/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.5748e-04\n",
      "Epoch 335/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3781e-04\n",
      "Epoch 336/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.1855e-04\n",
      "Epoch 337/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.9968e-04\n",
      "Epoch 338/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.8120e-04\n",
      "Epoch 339/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.6310e-04\n",
      "Epoch 340/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.4537e-04\n",
      "Epoch 341/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.2801e-04\n",
      "Epoch 342/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.1100e-04\n",
      "Epoch 343/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 7.9434e-04\n",
      "Epoch 344/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.7802e-04\n",
      "Epoch 345/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.6204e-04\n",
      "Epoch 346/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.4639e-04\n",
      "Epoch 347/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.3106e-04\n",
      "Epoch 348/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.1605e-04\n",
      "Epoch 349/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.0134e-04\n",
      "Epoch 350/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.8693e-04\n",
      "Epoch 351/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.7282e-04\n",
      "Epoch 352/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.5900e-04\n",
      "Epoch 353/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.4547e-04\n",
      "Epoch 354/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.3221e-04\n",
      "Epoch 355/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.1922e-04\n",
      "Epoch 356/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.0650e-04\n",
      "Epoch 357/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.9404e-04\n",
      "Epoch 358/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.8184e-04\n",
      "Epoch 359/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.6989e-04\n",
      "Epoch 360/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.5819e-04\n",
      "Epoch 361/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 5.4672e-04\n",
      "Epoch 362/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.3549e-04\n",
      "Epoch 363/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.2449e-04\n",
      "Epoch 364/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.1372e-04\n",
      "Epoch 365/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.0317e-04\n",
      "Epoch 366/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9283e-04\n",
      "Epoch 367/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8271e-04\n",
      "Epoch 368/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7280e-04\n",
      "Epoch 369/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.6308e-04\n",
      "Epoch 370/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5357e-04\n",
      "Epoch 371/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.4425e-04\n",
      "Epoch 372/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.3513e-04\n",
      "Epoch 373/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2619e-04\n",
      "Epoch 374/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.1744e-04\n",
      "Epoch 375/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.0886e-04\n",
      "Epoch 376/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.0047e-04\n",
      "Epoch 377/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.9224e-04\n",
      "Epoch 378/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.8418e-04\n",
      "Epoch 379/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.7629e-04\n",
      "Epoch 380/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.6856e-04\n",
      "Epoch 381/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.6099e-04\n",
      "Epoch 382/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.5357e-04\n",
      "Epoch 383/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.4631e-04\n",
      "Epoch 384/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.3920e-04\n",
      "Epoch 385/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 3.3223e-04\n",
      "Epoch 386/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.2541e-04\n",
      "Epoch 387/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.1872e-04\n",
      "Epoch 388/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.1218e-04\n",
      "Epoch 389/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.0576e-04\n",
      "Epoch 390/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.9948e-04\n",
      "Epoch 391/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.9333e-04\n",
      "Epoch 392/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.8731e-04\n",
      "Epoch 393/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.8140e-04\n",
      "Epoch 394/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.7563e-04\n",
      "Epoch 395/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.6996e-04\n",
      "Epoch 396/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.6442e-04\n",
      "Epoch 397/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.5899e-04\n",
      "Epoch 398/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.5367e-04\n",
      "Epoch 399/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.4846e-04\n",
      "Epoch 400/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.4335e-04\n",
      "Epoch 401/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.3836e-04\n",
      "Epoch 402/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.3346e-04\n",
      "Epoch 403/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2867e-04\n",
      "Epoch 404/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2397e-04\n",
      "Epoch 405/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.1937e-04\n",
      "Epoch 406/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1486e-04\n",
      "Epoch 407/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2.1045e-04\n",
      "Epoch 408/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.0612e-04\n",
      "Epoch 409/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.0189e-04\n",
      "Epoch 410/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.9774e-04\n",
      "Epoch 411/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.9368e-04\n",
      "Epoch 412/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.8970e-04\n",
      "Epoch 413/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.8581e-04\n",
      "Epoch 414/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.8199e-04\n",
      "Epoch 415/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7825e-04\n",
      "Epoch 416/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7459e-04\n",
      "Epoch 417/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7101e-04\n",
      "Epoch 418/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.6749e-04\n",
      "Epoch 419/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.6405e-04\n",
      "Epoch 420/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.6068e-04\n",
      "Epoch 421/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.5738e-04\n",
      "Epoch 422/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5415e-04\n",
      "Epoch 423/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5098e-04\n",
      "Epoch 424/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4788e-04\n",
      "Epoch 425/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4484e-04\n",
      "Epoch 426/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4187e-04\n",
      "Epoch 427/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3896e-04\n",
      "Epoch 428/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.3610e-04\n",
      "Epoch 429/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.3330e-04\n",
      "Epoch 430/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.3057e-04\n",
      "Epoch 431/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.2789e-04\n",
      "Epoch 432/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2526e-04\n",
      "Epoch 433/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2269e-04\n",
      "Epoch 434/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2016e-04\n",
      "Epoch 435/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1770e-04\n",
      "Epoch 436/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1528e-04\n",
      "Epoch 437/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.1291e-04\n",
      "Epoch 438/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1059e-04\n",
      "Epoch 439/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0832e-04\n",
      "Epoch 440/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0609e-04\n",
      "Epoch 441/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0392e-04\n",
      "Epoch 442/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0178e-04\n",
      "Epoch 443/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.9692e-05\n",
      "Epoch 444/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.7643e-05\n",
      "Epoch 445/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.5637e-05\n",
      "Epoch 446/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3672e-05\n",
      "Epoch 447/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.1748e-05\n",
      "Epoch 448/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.9863e-05\n",
      "Epoch 449/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.8018e-05\n",
      "Epoch 450/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.6211e-05\n",
      "Epoch 451/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.4439e-05\n",
      "Epoch 452/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2705e-05\n",
      "Epoch 453/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.1007e-05\n",
      "Epoch 454/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.9344e-05\n",
      "Epoch 455/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 7.7713e-05\n",
      "Epoch 456/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.6117e-05\n",
      "Epoch 457/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.4553e-05\n",
      "Epoch 458/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.3021e-05\n",
      "Epoch 459/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.1522e-05\n",
      "Epoch 460/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.0052e-05\n",
      "Epoch 461/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.8613e-05\n",
      "Epoch 462/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.7205e-05\n",
      "Epoch 463/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.5824e-05\n",
      "Epoch 464/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.4473e-05\n",
      "Epoch 465/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.3149e-05\n",
      "Epoch 466/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.1851e-05\n",
      "Epoch 467/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.0581e-05\n",
      "Epoch 468/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.9337e-05\n",
      "Epoch 469/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.8118e-05\n",
      "Epoch 470/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.6924e-05\n",
      "Epoch 471/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.5755e-05\n",
      "Epoch 472/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.4610e-05\n",
      "Epoch 473/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.3489e-05\n",
      "Epoch 474/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.2390e-05\n",
      "Epoch 475/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 5.1314e-05\n",
      "Epoch 476/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.0259e-05\n",
      "Epoch 477/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9227e-05\n",
      "Epoch 478/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.8216e-05\n",
      "Epoch 479/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.7225e-05\n",
      "Epoch 480/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6255e-05\n",
      "Epoch 481/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.5305e-05\n",
      "Epoch 482/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.4375e-05\n",
      "Epoch 483/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.3463e-05\n",
      "Epoch 484/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.2570e-05\n",
      "Epoch 485/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.1697e-05\n",
      "Epoch 486/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.0841e-05\n",
      "Epoch 487/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.0002e-05\n",
      "Epoch 488/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.9180e-05\n",
      "Epoch 489/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.8375e-05\n",
      "Epoch 490/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.7586e-05\n",
      "Epoch 491/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.6814e-05\n",
      "Epoch 492/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.6058e-05\n",
      "Epoch 493/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.5319e-05\n",
      "Epoch 494/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.4593e-05\n",
      "Epoch 495/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 3.3882e-05\n",
      "Epoch 496/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.3186e-05\n",
      "Epoch 497/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.2505e-05\n",
      "Epoch 498/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.1837e-05\n",
      "Epoch 499/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.1182e-05\n",
      "Epoch 500/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.0542e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x160aa34a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fit the model and train for 500 epochs\n",
    "model.fit(xs, ys, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "6LTIB5GgRuX7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "[[18.983877]]\n"
     ]
    }
   ],
   "source": [
    "# predict with value of 10\n",
    "print(model.predict(np.array([10.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Xg-EhY6TkVa"
   },
   "source": [
    "# II.2 Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Ad15862VTn38"
   },
   "outputs": [],
   "source": [
    "# for pytorch we need to convert the list to tensor\n",
    "xs = [[-1.0],  [0.0], [1.0], [2.0], [3.0], [4.0]]\n",
    "ys = [[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]]\n",
    "xs = Variable(torch.Tensor(xs))\n",
    "ys = Variable(torch.Tensor(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7B31YB7rUD6p"
   },
   "outputs": [],
   "source": [
    "# we define RegressionModel class with one linear layer and instanciate the model\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "mrlpsM4YUii3"
   },
   "outputs": [],
   "source": [
    "# Choose an appropriate loss function for regression (e.g., MSELoss)\n",
    "# and an optimizer (e.g., SGD) with a learning rate of 0.001.\n",
    "##<YOUR CODE>\n",
    "criterion = torch.nn.MSELoss() # Your code to define the loss function\n",
    "optimizer = torch.optim.SGD(lr=0.001, params=model.parameters()) # Your code to define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "-vj_erFoUvC7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 4.677433490753174\n",
      "epoch 1, loss 4.60520076751709\n",
      "epoch 2, loss 4.534523010253906\n",
      "epoch 3, loss 4.465365886688232\n",
      "epoch 4, loss 4.397695541381836\n",
      "epoch 5, loss 4.331478595733643\n",
      "epoch 6, loss 4.266682147979736\n",
      "epoch 7, loss 4.203275203704834\n",
      "epoch 8, loss 4.141226291656494\n",
      "epoch 9, loss 4.080504894256592\n",
      "epoch 10, loss 4.021081447601318\n",
      "epoch 11, loss 3.9629268646240234\n",
      "epoch 12, loss 3.906012535095215\n",
      "epoch 13, loss 3.850311517715454\n",
      "epoch 14, loss 3.7957961559295654\n",
      "epoch 15, loss 3.7424399852752686\n",
      "epoch 16, loss 3.690216064453125\n",
      "epoch 17, loss 3.6391007900238037\n",
      "epoch 18, loss 3.5890681743621826\n",
      "epoch 19, loss 3.540095329284668\n",
      "epoch 20, loss 3.492156744003296\n",
      "epoch 21, loss 3.4452311992645264\n",
      "epoch 22, loss 3.399294137954712\n",
      "epoch 23, loss 3.35432505607605\n",
      "epoch 24, loss 3.3103015422821045\n",
      "epoch 25, loss 3.267202377319336\n",
      "epoch 26, loss 3.2250072956085205\n",
      "epoch 27, loss 3.183696746826172\n",
      "epoch 28, loss 3.143249750137329\n",
      "epoch 29, loss 3.1036481857299805\n",
      "epoch 30, loss 3.0648725032806396\n",
      "epoch 31, loss 3.0269038677215576\n",
      "epoch 32, loss 2.98972487449646\n",
      "epoch 33, loss 2.953317880630493\n",
      "epoch 34, loss 2.917665719985962\n",
      "epoch 35, loss 2.8827507495880127\n",
      "epoch 36, loss 2.848557472229004\n",
      "epoch 37, loss 2.815070152282715\n",
      "epoch 38, loss 2.7822721004486084\n",
      "epoch 39, loss 2.750148057937622\n",
      "epoch 40, loss 2.7186832427978516\n",
      "epoch 41, loss 2.6878631114959717\n",
      "epoch 42, loss 2.6576731204986572\n",
      "epoch 43, loss 2.628099203109741\n",
      "epoch 44, loss 2.599128007888794\n",
      "epoch 45, loss 2.570744752883911\n",
      "epoch 46, loss 2.542938470840454\n",
      "epoch 47, loss 2.515695333480835\n",
      "epoch 48, loss 2.4890024662017822\n",
      "epoch 49, loss 2.4628474712371826\n",
      "epoch 50, loss 2.4372193813323975\n",
      "epoch 51, loss 2.4121055603027344\n",
      "epoch 52, loss 2.3874950408935547\n",
      "epoch 53, loss 2.363377094268799\n",
      "epoch 54, loss 2.339740037918091\n",
      "epoch 55, loss 2.316573143005371\n",
      "epoch 56, loss 2.2938661575317383\n",
      "epoch 57, loss 2.271609306335449\n",
      "epoch 58, loss 2.2497918605804443\n",
      "epoch 59, loss 2.2284045219421387\n",
      "epoch 60, loss 2.207437515258789\n",
      "epoch 61, loss 2.1868817806243896\n",
      "epoch 62, loss 2.1667277812957764\n",
      "epoch 63, loss 2.1469669342041016\n",
      "epoch 64, loss 2.1275908946990967\n",
      "epoch 65, loss 2.1085903644561768\n",
      "epoch 66, loss 2.0899574756622314\n",
      "epoch 67, loss 2.071683645248413\n",
      "epoch 68, loss 2.053762197494507\n",
      "epoch 69, loss 2.0361838340759277\n",
      "epoch 70, loss 2.018941640853882\n",
      "epoch 71, loss 2.002028226852417\n",
      "epoch 72, loss 1.985435962677002\n",
      "epoch 73, loss 1.96915864944458\n",
      "epoch 74, loss 1.9531883001327515\n",
      "epoch 75, loss 1.9375182390213013\n",
      "epoch 76, loss 1.9221420288085938\n",
      "epoch 77, loss 1.9070534706115723\n",
      "epoch 78, loss 1.8922457695007324\n",
      "epoch 79, loss 1.8777132034301758\n",
      "epoch 80, loss 1.863448977470398\n",
      "epoch 81, loss 1.8494478464126587\n",
      "epoch 82, loss 1.8357044458389282\n",
      "epoch 83, loss 1.8222123384475708\n",
      "epoch 84, loss 1.8089665174484253\n",
      "epoch 85, loss 1.7959604263305664\n",
      "epoch 86, loss 1.7831898927688599\n",
      "epoch 87, loss 1.7706489562988281\n",
      "epoch 88, loss 1.7583333253860474\n",
      "epoch 89, loss 1.7462376356124878\n",
      "epoch 90, loss 1.7343578338623047\n",
      "epoch 91, loss 1.7226877212524414\n",
      "epoch 92, loss 1.7112241983413696\n",
      "epoch 93, loss 1.69996178150177\n",
      "epoch 94, loss 1.6888965368270874\n",
      "epoch 95, loss 1.6780236959457397\n",
      "epoch 96, loss 1.6673394441604614\n",
      "epoch 97, loss 1.6568394899368286\n",
      "epoch 98, loss 1.6465201377868652\n",
      "epoch 99, loss 1.6363768577575684\n",
      "epoch 100, loss 1.6264063119888306\n",
      "epoch 101, loss 1.6166046857833862\n",
      "epoch 102, loss 1.606967568397522\n",
      "epoch 103, loss 1.5974926948547363\n",
      "epoch 104, loss 1.5881749391555786\n",
      "epoch 105, loss 1.579012393951416\n",
      "epoch 106, loss 1.5700007677078247\n",
      "epoch 107, loss 1.5611371994018555\n",
      "epoch 108, loss 1.5524178743362427\n",
      "epoch 109, loss 1.5438402891159058\n",
      "epoch 110, loss 1.5354009866714478\n",
      "epoch 111, loss 1.5270975828170776\n",
      "epoch 112, loss 1.518926739692688\n",
      "epoch 113, loss 1.5108851194381714\n",
      "epoch 114, loss 1.5029703378677368\n",
      "epoch 115, loss 1.4951802492141724\n",
      "epoch 116, loss 1.487511157989502\n",
      "epoch 117, loss 1.47996187210083\n",
      "epoch 118, loss 1.4725278615951538\n",
      "epoch 119, loss 1.465207576751709\n",
      "epoch 120, loss 1.4579992294311523\n",
      "epoch 121, loss 1.4508994817733765\n",
      "epoch 122, loss 1.443906307220459\n",
      "epoch 123, loss 1.4370173215866089\n",
      "epoch 124, loss 1.4302306175231934\n",
      "epoch 125, loss 1.4235434532165527\n",
      "epoch 126, loss 1.4169541597366333\n",
      "epoch 127, loss 1.4104610681533813\n",
      "epoch 128, loss 1.404060959815979\n",
      "epoch 129, loss 1.3977528810501099\n",
      "epoch 130, loss 1.3915343284606934\n",
      "epoch 131, loss 1.3854035139083862\n",
      "epoch 132, loss 1.3793586492538452\n",
      "epoch 133, loss 1.373397946357727\n",
      "epoch 134, loss 1.3675192594528198\n",
      "epoch 135, loss 1.3617210388183594\n",
      "epoch 136, loss 1.356001853942871\n",
      "epoch 137, loss 1.3503600358963013\n",
      "epoch 138, loss 1.3447933197021484\n",
      "epoch 139, loss 1.3393006324768066\n",
      "epoch 140, loss 1.33388090133667\n",
      "epoch 141, loss 1.3285316228866577\n",
      "epoch 142, loss 1.3232518434524536\n",
      "epoch 143, loss 1.3180400133132935\n",
      "epoch 144, loss 1.3128947019577026\n",
      "epoch 145, loss 1.307814359664917\n",
      "epoch 146, loss 1.3027979135513306\n",
      "epoch 147, loss 1.2978438138961792\n",
      "epoch 148, loss 1.292951226234436\n",
      "epoch 149, loss 1.2881182432174683\n",
      "epoch 150, loss 1.28334379196167\n",
      "epoch 151, loss 1.278626799583435\n",
      "epoch 152, loss 1.2739664316177368\n",
      "epoch 153, loss 1.2693606615066528\n",
      "epoch 154, loss 1.264809012413025\n",
      "epoch 155, loss 1.2603100538253784\n",
      "epoch 156, loss 1.255862832069397\n",
      "epoch 157, loss 1.2514663934707642\n",
      "epoch 158, loss 1.247119426727295\n",
      "epoch 159, loss 1.2428210973739624\n",
      "epoch 160, loss 1.2385705709457397\n",
      "epoch 161, loss 1.234366536140442\n",
      "epoch 162, loss 1.2302085161209106\n",
      "epoch 163, loss 1.2260948419570923\n",
      "epoch 164, loss 1.2220253944396973\n",
      "epoch 165, loss 1.2179985046386719\n",
      "epoch 166, loss 1.2140142917633057\n",
      "epoch 167, loss 1.2100708484649658\n",
      "epoch 168, loss 1.206168293952942\n",
      "epoch 169, loss 1.2023050785064697\n",
      "epoch 170, loss 1.1984810829162598\n",
      "epoch 171, loss 1.1946948766708374\n",
      "epoch 172, loss 1.1909459829330444\n",
      "epoch 173, loss 1.1872339248657227\n",
      "epoch 174, loss 1.183557391166687\n",
      "epoch 175, loss 1.179916262626648\n",
      "epoch 176, loss 1.1763092279434204\n",
      "epoch 177, loss 1.172736406326294\n",
      "epoch 178, loss 1.1691964864730835\n",
      "epoch 179, loss 1.1656889915466309\n",
      "epoch 180, loss 1.162213683128357\n",
      "epoch 181, loss 1.1587693691253662\n",
      "epoch 182, loss 1.1553553342819214\n",
      "epoch 183, loss 1.151971697807312\n",
      "epoch 184, loss 1.1486175060272217\n",
      "epoch 185, loss 1.1452919244766235\n",
      "epoch 186, loss 1.1419950723648071\n",
      "epoch 187, loss 1.138725757598877\n",
      "epoch 188, loss 1.1354836225509644\n",
      "epoch 189, loss 1.1322684288024902\n",
      "epoch 190, loss 1.1290794610977173\n",
      "epoch 191, loss 1.1259160041809082\n",
      "epoch 192, loss 1.1227779388427734\n",
      "epoch 193, loss 1.1196647882461548\n",
      "epoch 194, loss 1.1165755987167358\n",
      "epoch 195, loss 1.1135106086730957\n",
      "epoch 196, loss 1.110468864440918\n",
      "epoch 197, loss 1.107450246810913\n",
      "epoch 198, loss 1.1044540405273438\n",
      "epoch 199, loss 1.1014800071716309\n",
      "epoch 200, loss 1.0985277891159058\n",
      "epoch 201, loss 1.0955970287322998\n",
      "epoch 202, loss 1.0926872491836548\n",
      "epoch 203, loss 1.089797854423523\n",
      "epoch 204, loss 1.0869287252426147\n",
      "epoch 205, loss 1.084079623222351\n",
      "epoch 206, loss 1.0812498331069946\n",
      "epoch 207, loss 1.0784392356872559\n",
      "epoch 208, loss 1.0756475925445557\n",
      "epoch 209, loss 1.0728743076324463\n",
      "epoch 210, loss 1.0701191425323486\n",
      "epoch 211, loss 1.0673819780349731\n",
      "epoch 212, loss 1.064662218093872\n",
      "epoch 213, loss 1.0619597434997559\n",
      "epoch 214, loss 1.0592741966247559\n",
      "epoch 215, loss 1.0566052198410034\n",
      "epoch 216, loss 1.053952693939209\n",
      "epoch 217, loss 1.0513161420822144\n",
      "epoch 218, loss 1.048695683479309\n",
      "epoch 219, loss 1.0460904836654663\n",
      "epoch 220, loss 1.0435006618499756\n",
      "epoch 221, loss 1.0409258604049683\n",
      "epoch 222, loss 1.0383659601211548\n",
      "epoch 223, loss 1.0358203649520874\n",
      "epoch 224, loss 1.0332894325256348\n",
      "epoch 225, loss 1.0307722091674805\n",
      "epoch 226, loss 1.0282690525054932\n",
      "epoch 227, loss 1.0257792472839355\n",
      "epoch 228, loss 1.0233030319213867\n",
      "epoch 229, loss 1.020840048789978\n",
      "epoch 230, loss 1.0183900594711304\n",
      "epoch 231, loss 1.015952706336975\n",
      "epoch 232, loss 1.0135281085968018\n",
      "epoch 233, loss 1.0111157894134521\n",
      "epoch 234, loss 1.0087155103683472\n",
      "epoch 235, loss 1.006327509880066\n",
      "epoch 236, loss 1.003951072692871\n",
      "epoch 237, loss 1.0015864372253418\n",
      "epoch 238, loss 0.999233067035675\n",
      "epoch 239, loss 0.9968912601470947\n",
      "epoch 240, loss 0.9945603013038635\n",
      "epoch 241, loss 0.9922406077384949\n",
      "epoch 242, loss 0.9899314045906067\n",
      "epoch 243, loss 0.9876331686973572\n",
      "epoch 244, loss 0.9853453636169434\n",
      "epoch 245, loss 0.9830675721168518\n",
      "epoch 246, loss 0.980800211429596\n",
      "epoch 247, loss 0.9785428047180176\n",
      "epoch 248, loss 0.9762954115867615\n",
      "epoch 249, loss 0.974057674407959\n",
      "epoch 250, loss 0.9718295931816101\n",
      "epoch 251, loss 0.9696111083030701\n",
      "epoch 252, loss 0.967401921749115\n",
      "epoch 253, loss 0.9652020335197449\n",
      "epoch 254, loss 0.9630110859870911\n",
      "epoch 255, loss 0.9608290791511536\n",
      "epoch 256, loss 0.9586560726165771\n",
      "epoch 257, loss 0.9564918875694275\n",
      "epoch 258, loss 0.9543362259864807\n",
      "epoch 259, loss 0.9521892070770264\n",
      "epoch 260, loss 0.9500506520271301\n",
      "epoch 261, loss 0.9479203224182129\n",
      "epoch 262, loss 0.9457982182502747\n",
      "epoch 263, loss 0.9436842799186707\n",
      "epoch 264, loss 0.941578209400177\n",
      "epoch 265, loss 0.9394803643226624\n",
      "epoch 266, loss 0.9373900294303894\n",
      "epoch 267, loss 0.9353073239326477\n",
      "epoch 268, loss 0.9332324862480164\n",
      "epoch 269, loss 0.9311650395393372\n",
      "epoch 270, loss 0.9291050434112549\n",
      "epoch 271, loss 0.9270526766777039\n",
      "epoch 272, loss 0.9250072836875916\n",
      "epoch 273, loss 0.9229691624641418\n",
      "epoch 274, loss 0.9209381937980652\n",
      "epoch 275, loss 0.918914258480072\n",
      "epoch 276, loss 0.9168972373008728\n",
      "epoch 277, loss 0.9148869514465332\n",
      "epoch 278, loss 0.9128838181495667\n",
      "epoch 279, loss 0.9108870625495911\n",
      "epoch 280, loss 0.908897340297699\n",
      "epoch 281, loss 0.9069137573242188\n",
      "epoch 282, loss 0.9049369692802429\n",
      "epoch 283, loss 0.9029664397239685\n",
      "epoch 284, loss 0.9010024070739746\n",
      "epoch 285, loss 0.8990446925163269\n",
      "epoch 286, loss 0.8970933556556702\n",
      "epoch 287, loss 0.8951480388641357\n",
      "epoch 288, loss 0.8932088017463684\n",
      "epoch 289, loss 0.8912757039070129\n",
      "epoch 290, loss 0.8893487453460693\n",
      "epoch 291, loss 0.887427568435669\n",
      "epoch 292, loss 0.8855125308036804\n",
      "epoch 293, loss 0.883603036403656\n",
      "epoch 294, loss 0.8816993832588196\n",
      "epoch 295, loss 0.8798015713691711\n",
      "epoch 296, loss 0.877909243106842\n",
      "epoch 297, loss 0.8760226368904114\n",
      "epoch 298, loss 0.8741416335105896\n",
      "epoch 299, loss 0.8722661137580872\n",
      "epoch 300, loss 0.8703961968421936\n",
      "epoch 301, loss 0.8685315251350403\n",
      "epoch 302, loss 0.8666723370552063\n",
      "epoch 303, loss 0.8648185133934021\n",
      "epoch 304, loss 0.8629700541496277\n",
      "epoch 305, loss 0.8611266613006592\n",
      "epoch 306, loss 0.8592886328697205\n",
      "epoch 307, loss 0.8574557304382324\n",
      "epoch 308, loss 0.8556279540061951\n",
      "epoch 309, loss 0.8538051247596741\n",
      "epoch 310, loss 0.8519874215126038\n",
      "epoch 311, loss 0.8501748442649841\n",
      "epoch 312, loss 0.8483670353889465\n",
      "epoch 313, loss 0.8465642929077148\n",
      "epoch 314, loss 0.8447663187980652\n",
      "epoch 315, loss 0.8429732322692871\n",
      "epoch 316, loss 0.8411850333213806\n",
      "epoch 317, loss 0.8394015431404114\n",
      "epoch 318, loss 0.8376230597496033\n",
      "epoch 319, loss 0.8358489871025085\n",
      "epoch 320, loss 0.8340797424316406\n",
      "epoch 321, loss 0.8323150277137756\n",
      "epoch 322, loss 0.8305549025535583\n",
      "epoch 323, loss 0.8287994861602783\n",
      "epoch 324, loss 0.8270487785339355\n",
      "epoch 325, loss 0.8253023624420166\n",
      "epoch 326, loss 0.8235604166984558\n",
      "epoch 327, loss 0.8218230605125427\n",
      "epoch 328, loss 0.8200900554656982\n",
      "epoch 329, loss 0.8183615207672119\n",
      "epoch 330, loss 0.8166373372077942\n",
      "epoch 331, loss 0.8149175047874451\n",
      "epoch 332, loss 0.8132020831108093\n",
      "epoch 333, loss 0.8114907741546631\n",
      "epoch 334, loss 0.8097838759422302\n",
      "epoch 335, loss 0.808081328868866\n",
      "epoch 336, loss 0.8063827157020569\n",
      "epoch 337, loss 0.8046886324882507\n",
      "epoch 338, loss 0.8029983639717102\n",
      "epoch 339, loss 0.8013125061988831\n",
      "epoch 340, loss 0.7996308207511902\n",
      "epoch 341, loss 0.7979530692100525\n",
      "epoch 342, loss 0.7962793707847595\n",
      "epoch 343, loss 0.7946096062660217\n",
      "epoch 344, loss 0.7929442524909973\n",
      "epoch 345, loss 0.7912827134132385\n",
      "epoch 346, loss 0.789625346660614\n",
      "epoch 347, loss 0.7879716753959656\n",
      "epoch 348, loss 0.7863220572471619\n",
      "epoch 349, loss 0.7846763730049133\n",
      "epoch 350, loss 0.7830347418785095\n",
      "epoch 351, loss 0.7813968062400818\n",
      "epoch 352, loss 0.7797628045082092\n",
      "epoch 353, loss 0.7781327366828918\n",
      "epoch 354, loss 0.776506245136261\n",
      "epoch 355, loss 0.7748839259147644\n",
      "epoch 356, loss 0.7732651829719543\n",
      "epoch 357, loss 0.7716503143310547\n",
      "epoch 358, loss 0.7700390815734863\n",
      "epoch 359, loss 0.7684317231178284\n",
      "epoch 360, loss 0.7668280601501465\n",
      "epoch 361, loss 0.7652280926704407\n",
      "epoch 362, loss 0.7636319994926453\n",
      "epoch 363, loss 0.7620394229888916\n",
      "epoch 364, loss 0.7604503631591797\n",
      "epoch 365, loss 0.7588651776313782\n",
      "epoch 366, loss 0.7572835087776184\n",
      "epoch 367, loss 0.7557055950164795\n",
      "epoch 368, loss 0.7541310787200928\n",
      "epoch 369, loss 0.7525603771209717\n",
      "epoch 370, loss 0.750993013381958\n",
      "epoch 371, loss 0.7494292855262756\n",
      "epoch 372, loss 0.7478691935539246\n",
      "epoch 373, loss 0.7463123798370361\n",
      "epoch 374, loss 0.7447593212127686\n",
      "epoch 375, loss 0.7432098388671875\n",
      "epoch 376, loss 0.7416635155677795\n",
      "epoch 377, loss 0.7401209473609924\n",
      "epoch 378, loss 0.7385818362236023\n",
      "epoch 379, loss 0.73704594373703\n",
      "epoch 380, loss 0.73551344871521\n",
      "epoch 381, loss 0.733984649181366\n",
      "epoch 382, loss 0.7324590682983398\n",
      "epoch 383, loss 0.7309369444847107\n",
      "epoch 384, loss 0.7294180393218994\n",
      "epoch 385, loss 0.7279026508331299\n",
      "epoch 386, loss 0.7263906598091125\n",
      "epoch 387, loss 0.7248820662498474\n",
      "epoch 388, loss 0.7233765721321106\n",
      "epoch 389, loss 0.721874475479126\n",
      "epoch 390, loss 0.7203757166862488\n",
      "epoch 391, loss 0.7188804149627686\n",
      "epoch 392, loss 0.7173881530761719\n",
      "epoch 393, loss 0.7158994078636169\n",
      "epoch 394, loss 0.7144137024879456\n",
      "epoch 395, loss 0.7129313349723816\n",
      "epoch 396, loss 0.7114521861076355\n",
      "epoch 397, loss 0.7099762558937073\n",
      "epoch 398, loss 0.7085036635398865\n",
      "epoch 399, loss 0.7070342898368835\n",
      "epoch 400, loss 0.7055678963661194\n",
      "epoch 401, loss 0.7041048407554626\n",
      "epoch 402, loss 0.7026450037956238\n",
      "epoch 403, loss 0.7011882662773132\n",
      "epoch 404, loss 0.6997345089912415\n",
      "epoch 405, loss 0.6982841491699219\n",
      "epoch 406, loss 0.6968369483947754\n",
      "epoch 407, loss 0.6953927874565125\n",
      "epoch 408, loss 0.6939516663551331\n",
      "epoch 409, loss 0.692513644695282\n",
      "epoch 410, loss 0.6910788416862488\n",
      "epoch 411, loss 0.6896470189094543\n",
      "epoch 412, loss 0.6882184147834778\n",
      "epoch 413, loss 0.68679279088974\n",
      "epoch 414, loss 0.6853702068328857\n",
      "epoch 415, loss 0.6839508414268494\n",
      "epoch 416, loss 0.6825342774391174\n",
      "epoch 417, loss 0.6811209321022034\n",
      "epoch 418, loss 0.6797105669975281\n",
      "epoch 419, loss 0.678303062915802\n",
      "epoch 420, loss 0.6768987774848938\n",
      "epoch 421, loss 0.6754975318908691\n",
      "epoch 422, loss 0.6740991473197937\n",
      "epoch 423, loss 0.6727038025856018\n",
      "epoch 424, loss 0.6713113188743591\n",
      "epoch 425, loss 0.6699219346046448\n",
      "epoch 426, loss 0.6685354709625244\n",
      "epoch 427, loss 0.6671519875526428\n",
      "epoch 428, loss 0.6657713055610657\n",
      "epoch 429, loss 0.6643936634063721\n",
      "epoch 430, loss 0.6630188822746277\n",
      "epoch 431, loss 0.6616470217704773\n",
      "epoch 432, loss 0.6602782607078552\n",
      "epoch 433, loss 0.658912181854248\n",
      "epoch 434, loss 0.6575491428375244\n",
      "epoch 435, loss 0.6561889052391052\n",
      "epoch 436, loss 0.6548315286636353\n",
      "epoch 437, loss 0.653477132320404\n",
      "epoch 438, loss 0.6521255373954773\n",
      "epoch 439, loss 0.6507769227027893\n",
      "epoch 440, loss 0.6494309306144714\n",
      "epoch 441, loss 0.6480879187583923\n",
      "epoch 442, loss 0.6467476487159729\n",
      "epoch 443, loss 0.6454102993011475\n",
      "epoch 444, loss 0.6440756916999817\n",
      "epoch 445, loss 0.6427440643310547\n",
      "epoch 446, loss 0.6414151191711426\n",
      "epoch 447, loss 0.6400889754295349\n",
      "epoch 448, loss 0.6387656331062317\n",
      "epoch 449, loss 0.6374450325965881\n",
      "epoch 450, loss 0.636127233505249\n",
      "epoch 451, loss 0.6348122358322144\n",
      "epoch 452, loss 0.6334999799728394\n",
      "epoch 453, loss 0.632190465927124\n",
      "epoch 454, loss 0.6308837532997131\n",
      "epoch 455, loss 0.6295798420906067\n",
      "epoch 456, loss 0.6282786130905151\n",
      "epoch 457, loss 0.6269801259040833\n",
      "epoch 458, loss 0.6256843209266663\n",
      "epoch 459, loss 0.6243911981582642\n",
      "epoch 460, loss 0.6231008172035217\n",
      "epoch 461, loss 0.621813178062439\n",
      "epoch 462, loss 0.6205282807350159\n",
      "epoch 463, loss 0.6192460060119629\n",
      "epoch 464, loss 0.6179662346839905\n",
      "epoch 465, loss 0.6166895031929016\n",
      "epoch 466, loss 0.6154151558876038\n",
      "epoch 467, loss 0.6141436696052551\n",
      "epoch 468, loss 0.6128746867179871\n",
      "epoch 469, loss 0.6116083860397339\n",
      "epoch 470, loss 0.6103448271751404\n",
      "epoch 471, loss 0.6090838313102722\n",
      "epoch 472, loss 0.6078255772590637\n",
      "epoch 473, loss 0.6065698266029358\n",
      "epoch 474, loss 0.6053166389465332\n",
      "epoch 475, loss 0.6040663123130798\n",
      "epoch 476, loss 0.6028183102607727\n",
      "epoch 477, loss 0.6015729904174805\n",
      "epoch 478, loss 0.6003304123878479\n",
      "epoch 479, loss 0.5990903973579407\n",
      "epoch 480, loss 0.5978529453277588\n",
      "epoch 481, loss 0.5966179966926575\n",
      "epoch 482, loss 0.5953856110572815\n",
      "epoch 483, loss 0.5941559076309204\n",
      "epoch 484, loss 0.5929285883903503\n",
      "epoch 485, loss 0.5917040109634399\n",
      "epoch 486, loss 0.5904819369316101\n",
      "epoch 487, loss 0.5892624258995056\n",
      "epoch 488, loss 0.5880454182624817\n",
      "epoch 489, loss 0.5868309140205383\n",
      "epoch 490, loss 0.5856189131736755\n",
      "epoch 491, loss 0.5844094157218933\n",
      "epoch 492, loss 0.5832026600837708\n",
      "epoch 493, loss 0.5819981694221497\n",
      "epoch 494, loss 0.5807963609695435\n",
      "epoch 495, loss 0.5795968770980835\n",
      "epoch 496, loss 0.5784000754356384\n",
      "epoch 497, loss 0.5772055983543396\n",
      "epoch 498, loss 0.5760136246681213\n",
      "epoch 499, loss 0.5748242735862732\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(500):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing\n",
    "    # x to the model\n",
    "    pred_y = model(xs)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(pred_y, ys)\n",
    "\n",
    "    # Zero gradients, perform a backward pass,\n",
    "    # and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Ag4QEF-gU6_4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.74264144897461\n"
     ]
    }
   ],
   "source": [
    "new_var = Variable(torch.Tensor([[10.0]]))\n",
    "pred_y = model(new_var)\n",
    "print(model(new_var).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlmKn7mgVANd"
   },
   "source": [
    "# **Questions:**\n",
    "\n",
    "Compare and contrast the performance of the TensorFlow and PyTorch models. Consider the following:\n",
    "\n",
    "- Accuracy: Which model achieved higher accuracy in this prediction? Quantify the difference if possible.\n",
    "\n",
    "- Complexity: Briefly discuss the relative complexity of implementing the models in each framework based on your experience. Do you think the difference in accuracy justifies any potential increase in complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hziJGClzHDQ"
   },
   "source": [
    "References:\n",
    "\n",
    "- [Pytorch and Tensorflow History and design philosophy](https://github.com/Seeed-Projects/reComputer-Jetson-for-Beginners/blob/main/3-Basic-Tools-and-Getting-Started/3.3-Pytorch-and-Tensorflow/README.md)\n",
    "- [Pytorch installation guide](https://pytorch.org/)\n",
    "- [Tensorflow installation guide](https://www.tensorflow.org/install?hl=fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzDqc2KT0DP2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMdA6dWPS+NZrpBKSV48EZ4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lab3-deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
