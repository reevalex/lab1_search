{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NZ1cPpAtS_R"
   },
   "source": [
    "![LiU_logo.png](data:image/png;base64,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)\n",
    "## Linkoping University: TDDC17 Artificial Intelligence\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSKKEe1vtl2_"
   },
   "source": [
    "**Lab5 : Deep Learning**\n",
    "\n",
    "**Lab5.1 : Understading the basic of Pytorch and Tensorflow framework**\n",
    "\n",
    "  In the field of deep learning, PyTorch and TensorFlow are two of the most popular frameworks. While both are used for building and training neural networks, they have different histories and design philosophies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I-wbE63zOeH"
   },
   "source": [
    "**Relationship Between PyTorch and TensorFlow**\n",
    "\n",
    "Despite being competitors, PyTorch and TensorFlow share many similarities and have influenced each other. Here are some key points of their relationship:\n",
    "\n",
    "* Design Philosophy: PyTorch focuses on flexibility and dynamic computation graphs, making it suitable for research and prototyping. In contrast, TensorFlow emphasizes performance and static computation graphs, which are ideal for production environments. TensorFlow 2.0 introduced Eager Execution mode, which partially draws from PyTorch's design philosophy.\n",
    "\n",
    "* Community and Ecosystem: Both frameworks boast large communities and rich ecosystems, offering numerous models and tools. TensorFlow's TensorFlow Hub and PyTorch's PyTorch Hub are excellent platforms for sharing pre-trained models.\n",
    "\n",
    "* Interoperability: Both frameworks are continually improving their interoperability. For instance, the ONNX (Open Neural Network Exchange) format allows model conversion between PyTorch and TensorFlow, enhancing compatibility and collaboration between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYBnz_DhFLCl"
   },
   "source": [
    "**I. Pytorch and Tensorflow Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee-nZMDqtCBl"
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPIeN0yJG1CK"
   },
   "source": [
    "# I.1 Pytorch Hand-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "llxEVMREG88J"
   },
   "outputs": [],
   "source": [
    "## import torch package\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "yzsJxfVBHIiD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a tensor of size 2x3x4\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "LbK_6ImcHwk6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the size of the tensor\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "6C-dGqwKH0xU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point in a 24 dimensional space\n",
      "organised in 3 sub-dimensions\n"
     ]
    }
   ],
   "source": [
    "# prints dimensional space and sub-dimensions\n",
    "print(f'point in a {t.numel()} dimensional space')\n",
    "print(f'organised in {t.dim()} sub-dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "n1djyiCMII63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This resizes the tensor permanently\n",
    "r = torch.Tensor(t)\n",
    "r.resize_(3, 8)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "zxOL3M4wIeA3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a 1D tensor of integers 1 to 4\n",
    "v = torch.Tensor([1, 2, 3, 4])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "l8Upm2oVItFN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 1, size: 4\n"
     ]
    }
   ],
   "source": [
    "# Print number of dimensions (1D) and size of tensor\n",
    "print(f'dim: {v.dim()}, size: {v.size()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "-aJopdypIv6P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 2., 0.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.Tensor([1, 0, 2, 0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "76CGG3xaIzj9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 6., 0.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "v * w\n",
    "# 1, 0, 6, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "kePuuavbI5tS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar product: 1*1 + 2*0 + 3*2 + 4*0\n",
    "v @ w\n",
    "# 1, 0, 6, 0 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "ntSIfDgoJArg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  4.,  9., 16.]) tensor([1., 2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "# Square all elements in the tensor\n",
    "print(v.pow(2), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "nwOaqL1qJO05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2x4 tensor(matrix 2D)\n",
    "m = torch.Tensor([[2, 5, 3, 7],\n",
    "                  [4, 2, 1, 9]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "RhDvnwV5JXEa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "hwXGqBJwJdBI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "m[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "YZbYw2iWJgrO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing row 0, column 2 (0-indexed)\n",
    "m[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "op3juZ2oKCva"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.7000,  2.7000, -0.3000])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor from 5.7 to -2.1 with each having a space of -3\n",
    "torch.arange(5.7, -2.1, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "XmbttCKQKLlZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0000, 3.2632, 3.5263, 3.7895, 4.0526, 4.3158, 4.5789, 4.8421, 5.1053,\n",
       "         5.3684, 5.6316, 5.8947, 6.1579, 6.4211, 6.6842, 6.9474, 7.2105, 7.4737,\n",
       "         7.7368, 8.0000]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns a 1D tensor of steps equally spaced points between start=3, end=8 and steps=20\n",
    "torch.linspace(3, 8, 20).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "r5db5M3uKOYa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with 0's\n",
    "torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "wQzh3l27KRa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor filled with 1's\n",
    "torch.ones(3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "dY2ePI3HK1f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 5., 3., 7.],\n",
       "        [4., 2., 1., 9.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move your tensor to GPU device 0 if there is one (first GPU in the system)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "m.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqkbIYkIKcVu"
   },
   "source": [
    "# I.1 Tensorflow Hand-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "ugdHTPyDLhfV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "d6Iv0C4zLqxd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow constant\n",
    "tensor_constant = tf.constant([[1, 2], [3, 4]])\n",
    "print(tensor_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "zGOop-qfL9fh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
      "array([[5, 6],\n",
      "       [7, 8]], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorFlow variable\n",
    "tensor_variable = tf.Variable([[5, 6], [7, 8]])\n",
    "print(tensor_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "E6rpJc5xMEwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6  8]\n",
      " [10 12]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Perform element-wise addition\n",
    "tensor_sum = tf.add(tensor_constant, tensor_variable)\n",
    "print(tensor_sum)\n",
    "# 6, 8, 10, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "Z6nB5kCYMIz8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[19 22]\n",
      " [43 50]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Perform matrix multiplication\n",
    "tensor_product = tf.matmul(tensor_constant, tensor_variable)\n",
    "print(tensor_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "vAIX8x99MMPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]], shape=(4, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Reshape a tensor\n",
    "tensor_reshaped = tf.reshape(tensor_constant, [4, 1])\n",
    "print(tensor_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "R4gzwdsNMSnp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1  4]\n",
      " [ 9 16]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Apply a function to a tensor element-wise\n",
    "tensor_squared = tf.square(tensor_constant)\n",
    "print(tensor_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "fHSTuNFfMW_V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a TensorFlow tensor to a NumPy array\n",
    "numpy_array = tensor_constant.numpy()\n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07UjMj61NRY9"
   },
   "source": [
    "##II Linear regression with tensorflow and pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "YP1ndz2ROmo5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8b2zjLJTSFh"
   },
   "source": [
    "Given a simple linear regression problem\n",
    "\n",
    "x= [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "y= [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]\n",
    "\n",
    "y = 2*x -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blmz0zJdPKtg"
   },
   "source": [
    "# II.1 Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "z8u97phwO6Ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reevalex/Master/Term-1/TDDC17/Group/lab3_deep_learning/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# we design our model using the Sequential, which is a linear stack of layers. In this model, we use only one layer(neuron)\n",
    "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "sGniRaaIPcKR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.src.optimizers.sgd.SGD at 0x17a14b560>, 'mean_squared_error')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer and loss functions to train our Neural Network model. In this, we use SGD optimizer and MSE as our loss function\n",
    "# TODO: Implement the model compilation step using the appropriate optimizer and loss function\n",
    "# The optimizer should be 'sgd' and the loss function should be 'mean_squared_error'\n",
    "# Hint: Use the model.compile() method\n",
    "\n",
    "##<YOUR CODE>\n",
    "model.compile(\n",
    "    optimizer=\"sgd\",\n",
    "    loss=\"mean_squared_error\"\n",
    ")\n",
    "\n",
    "model.optimizer, model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "tw-kNpRURVbW"
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "MEGmgNxqRbl7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 49.2584\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 39.1454\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 31.1809\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 24.9070\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 19.9632\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 16.0662\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 12.9928\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 10.5675\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 8.6524\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 7.1386\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.9409\n",
      "Epoch 12/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.9920\n",
      "Epoch 13/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 4.2388\n",
      "Epoch 14/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.6399\n",
      "Epoch 15/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 3.1624\n",
      "Epoch 16/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.7806\n",
      "Epoch 17/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.4742\n",
      "Epoch 18/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2273\n",
      "Epoch 19/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.0273\n",
      "Epoch 20/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.8642\n",
      "Epoch 21/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.7304\n",
      "Epoch 22/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6198\n",
      "Epoch 23/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5274\n",
      "Epoch 24/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.4495\n",
      "Epoch 25/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.3831\n",
      "Epoch 26/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3259\n",
      "Epoch 27/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2760\n",
      "Epoch 28/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.2320\n",
      "Epoch 29/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.1927\n",
      "Epoch 30/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.1572\n",
      "Epoch 31/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.1247\n",
      "Epoch 32/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0948\n",
      "Epoch 33/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0669\n",
      "Epoch 34/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0408\n",
      "Epoch 35/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0161\n",
      "Epoch 36/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.9926\n",
      "Epoch 37/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.9701\n",
      "Epoch 38/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.9486\n",
      "Epoch 39/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.9278\n",
      "Epoch 40/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.9078\n",
      "Epoch 41/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8883\n",
      "Epoch 42/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8695\n",
      "Epoch 43/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8511\n",
      "Epoch 44/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.8333\n",
      "Epoch 45/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8158\n",
      "Epoch 46/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7989\n",
      "Epoch 47/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7823\n",
      "Epoch 48/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7660\n",
      "Epoch 49/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7502\n",
      "Epoch 50/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7347\n",
      "Epoch 51/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7195\n",
      "Epoch 52/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7047\n",
      "Epoch 53/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6902\n",
      "Epoch 54/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6760\n",
      "Epoch 55/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6620\n",
      "Epoch 56/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6484\n",
      "Epoch 57/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6351\n",
      "Epoch 58/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.6220\n",
      "Epoch 59/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6092\n",
      "Epoch 60/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5967\n",
      "Epoch 61/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5845\n",
      "Epoch 62/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5725\n",
      "Epoch 63/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5607\n",
      "Epoch 64/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5492\n",
      "Epoch 65/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5379\n",
      "Epoch 66/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5268\n",
      "Epoch 67/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5160\n",
      "Epoch 68/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5054\n",
      "Epoch 69/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4950\n",
      "Epoch 70/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4849\n",
      "Epoch 71/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.4749\n",
      "Epoch 72/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4651\n",
      "Epoch 73/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4556\n",
      "Epoch 74/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.4462\n",
      "Epoch 75/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4371\n",
      "Epoch 76/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4281\n",
      "Epoch 77/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4193\n",
      "Epoch 78/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4107\n",
      "Epoch 79/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4022\n",
      "Epoch 80/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3940\n",
      "Epoch 81/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3859\n",
      "Epoch 82/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3780\n",
      "Epoch 83/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3702\n",
      "Epoch 84/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3626\n",
      "Epoch 85/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3551\n",
      "Epoch 86/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3479\n",
      "Epoch 87/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3407\n",
      "Epoch 88/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3337\n",
      "Epoch 89/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3269\n",
      "Epoch 90/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3201\n",
      "Epoch 91/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3136\n",
      "Epoch 92/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3071\n",
      "Epoch 93/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3008\n",
      "Epoch 94/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2946\n",
      "Epoch 95/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2886\n",
      "Epoch 96/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2827\n",
      "Epoch 97/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2769\n",
      "Epoch 98/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2712\n",
      "Epoch 99/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2656\n",
      "Epoch 100/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2601\n",
      "Epoch 101/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2548\n",
      "Epoch 102/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2496\n",
      "Epoch 103/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2444\n",
      "Epoch 104/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2394\n",
      "Epoch 105/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2345\n",
      "Epoch 106/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2297\n",
      "Epoch 107/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2250\n",
      "Epoch 108/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2203\n",
      "Epoch 109/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2158\n",
      "Epoch 110/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2114\n",
      "Epoch 111/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2070\n",
      "Epoch 112/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2028\n",
      "Epoch 113/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1986\n",
      "Epoch 114/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1945\n",
      "Epoch 115/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1905\n",
      "Epoch 116/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1866\n",
      "Epoch 117/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1828\n",
      "Epoch 118/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1790\n",
      "Epoch 119/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1754\n",
      "Epoch 120/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1718\n",
      "Epoch 121/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1682\n",
      "Epoch 122/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1648\n",
      "Epoch 123/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1614\n",
      "Epoch 124/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1581\n",
      "Epoch 125/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1548\n",
      "Epoch 126/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1517\n",
      "Epoch 127/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1485\n",
      "Epoch 128/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1455\n",
      "Epoch 129/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1425\n",
      "Epoch 130/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1396\n",
      "Epoch 131/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1367\n",
      "Epoch 132/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1339\n",
      "Epoch 133/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1311\n",
      "Epoch 134/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1285\n",
      "Epoch 135/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1258\n",
      "Epoch 136/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1232\n",
      "Epoch 137/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1207\n",
      "Epoch 138/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1182\n",
      "Epoch 139/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1158\n",
      "Epoch 140/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1134\n",
      "Epoch 141/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1111\n",
      "Epoch 142/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1088\n",
      "Epoch 143/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1066\n",
      "Epoch 144/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1044\n",
      "Epoch 145/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1022\n",
      "Epoch 146/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1001\n",
      "Epoch 147/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0981\n",
      "Epoch 148/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0961\n",
      "Epoch 149/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0941\n",
      "Epoch 150/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0922\n",
      "Epoch 151/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0903\n",
      "Epoch 152/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0884\n",
      "Epoch 153/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0866\n",
      "Epoch 154/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0848\n",
      "Epoch 155/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0831\n",
      "Epoch 156/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0814\n",
      "Epoch 157/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0797\n",
      "Epoch 158/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0781\n",
      "Epoch 159/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0765\n",
      "Epoch 160/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0749\n",
      "Epoch 161/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0733\n",
      "Epoch 162/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0718\n",
      "Epoch 163/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0704\n",
      "Epoch 164/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0689\n",
      "Epoch 165/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0675\n",
      "Epoch 166/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0661\n",
      "Epoch 167/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0648\n",
      "Epoch 168/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0634\n",
      "Epoch 169/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0621\n",
      "Epoch 170/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0608\n",
      "Epoch 171/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0596\n",
      "Epoch 172/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0584\n",
      "Epoch 173/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0572\n",
      "Epoch 174/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0560\n",
      "Epoch 175/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0549\n",
      "Epoch 176/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0537\n",
      "Epoch 177/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0526\n",
      "Epoch 178/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0515\n",
      "Epoch 179/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0505\n",
      "Epoch 180/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0494\n",
      "Epoch 181/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0484\n",
      "Epoch 182/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0474\n",
      "Epoch 183/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0465\n",
      "Epoch 184/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0455\n",
      "Epoch 185/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0446\n",
      "Epoch 186/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0437\n",
      "Epoch 187/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0428\n",
      "Epoch 188/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0419\n",
      "Epoch 189/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0410\n",
      "Epoch 190/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0402\n",
      "Epoch 191/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0394\n",
      "Epoch 192/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0385\n",
      "Epoch 193/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0378\n",
      "Epoch 194/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0370\n",
      "Epoch 195/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0362\n",
      "Epoch 196/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0355\n",
      "Epoch 197/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0347\n",
      "Epoch 198/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0340\n",
      "Epoch 199/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0333\n",
      "Epoch 200/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0326\n",
      "Epoch 201/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0320\n",
      "Epoch 202/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0313\n",
      "Epoch 203/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0307\n",
      "Epoch 204/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0300\n",
      "Epoch 205/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0294\n",
      "Epoch 206/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0288\n",
      "Epoch 207/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0282\n",
      "Epoch 208/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0277\n",
      "Epoch 209/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0271\n",
      "Epoch 210/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0265\n",
      "Epoch 211/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0260\n",
      "Epoch 212/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0254\n",
      "Epoch 213/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0249\n",
      "Epoch 214/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0244\n",
      "Epoch 215/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0239\n",
      "Epoch 216/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0234\n",
      "Epoch 217/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0229\n",
      "Epoch 218/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0225\n",
      "Epoch 219/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0220\n",
      "Epoch 220/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0216\n",
      "Epoch 221/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0211\n",
      "Epoch 222/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0207\n",
      "Epoch 223/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0203\n",
      "Epoch 224/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0198\n",
      "Epoch 225/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0194\n",
      "Epoch 226/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0190\n",
      "Epoch 227/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0186\n",
      "Epoch 228/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0183\n",
      "Epoch 229/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0179\n",
      "Epoch 230/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0175\n",
      "Epoch 231/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0172\n",
      "Epoch 232/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0168\n",
      "Epoch 233/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0165\n",
      "Epoch 234/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0161\n",
      "Epoch 235/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0158\n",
      "Epoch 236/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0155\n",
      "Epoch 237/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0151\n",
      "Epoch 238/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0148\n",
      "Epoch 239/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0145\n",
      "Epoch 240/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0142\n",
      "Epoch 241/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0139\n",
      "Epoch 242/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0137\n",
      "Epoch 243/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0134\n",
      "Epoch 244/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0131\n",
      "Epoch 245/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0128\n",
      "Epoch 246/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0126\n",
      "Epoch 247/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0123\n",
      "Epoch 248/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0121\n",
      "Epoch 249/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0118\n",
      "Epoch 250/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0116\n",
      "Epoch 251/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0113\n",
      "Epoch 252/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0111\n",
      "Epoch 253/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0109\n",
      "Epoch 254/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0106\n",
      "Epoch 255/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0104\n",
      "Epoch 256/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0102\n",
      "Epoch 257/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0100\n",
      "Epoch 258/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0098\n",
      "Epoch 259/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0096\n",
      "Epoch 260/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0094\n",
      "Epoch 261/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0092\n",
      "Epoch 262/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0090\n",
      "Epoch 263/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0088\n",
      "Epoch 264/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0086\n",
      "Epoch 265/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0085\n",
      "Epoch 266/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0083\n",
      "Epoch 267/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0081\n",
      "Epoch 268/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0080\n",
      "Epoch 269/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0078\n",
      "Epoch 270/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0076\n",
      "Epoch 271/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0075\n",
      "Epoch 272/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0073\n",
      "Epoch 273/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0072\n",
      "Epoch 274/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0070\n",
      "Epoch 275/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0069\n",
      "Epoch 276/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0067\n",
      "Epoch 277/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0066\n",
      "Epoch 278/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0065\n",
      "Epoch 279/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0063\n",
      "Epoch 280/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0062\n",
      "Epoch 281/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0061\n",
      "Epoch 282/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0060\n",
      "Epoch 283/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0058\n",
      "Epoch 284/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0057\n",
      "Epoch 285/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0056\n",
      "Epoch 286/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0055\n",
      "Epoch 287/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0054\n",
      "Epoch 288/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0053\n",
      "Epoch 289/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0051\n",
      "Epoch 290/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0050\n",
      "Epoch 291/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0049\n",
      "Epoch 292/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0048\n",
      "Epoch 293/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0047\n",
      "Epoch 294/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046\n",
      "Epoch 295/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0045\n",
      "Epoch 296/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0045\n",
      "Epoch 297/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0044\n",
      "Epoch 298/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0043\n",
      "Epoch 299/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0042\n",
      "Epoch 300/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0041\n",
      "Epoch 301/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0040\n",
      "Epoch 302/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0039\n",
      "Epoch 303/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0038\n",
      "Epoch 304/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0038\n",
      "Epoch 305/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0037\n",
      "Epoch 306/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0036\n",
      "Epoch 307/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035\n",
      "Epoch 308/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0035\n",
      "Epoch 309/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0034\n",
      "Epoch 310/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0033\n",
      "Epoch 311/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0033\n",
      "Epoch 312/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0032\n",
      "Epoch 313/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0031\n",
      "Epoch 314/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0031\n",
      "Epoch 315/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0030\n",
      "Epoch 316/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0029\n",
      "Epoch 317/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029\n",
      "Epoch 318/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0028\n",
      "Epoch 319/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0028\n",
      "Epoch 320/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0027\n",
      "Epoch 321/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026\n",
      "Epoch 322/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026\n",
      "Epoch 323/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0025\n",
      "Epoch 324/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025\n",
      "Epoch 325/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024\n",
      "Epoch 326/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0024\n",
      "Epoch 327/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023\n",
      "Epoch 328/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0023\n",
      "Epoch 329/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022\n",
      "Epoch 330/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0022\n",
      "Epoch 331/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0022\n",
      "Epoch 332/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021\n",
      "Epoch 333/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0021\n",
      "Epoch 334/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020\n",
      "Epoch 335/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0020\n",
      "Epoch 336/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019\n",
      "Epoch 337/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019\n",
      "Epoch 338/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0019\n",
      "Epoch 339/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018\n",
      "Epoch 340/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0018\n",
      "Epoch 341/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017\n",
      "Epoch 342/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017\n",
      "Epoch 343/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0017\n",
      "Epoch 344/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016\n",
      "Epoch 345/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016\n",
      "Epoch 346/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0016\n",
      "Epoch 347/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0015\n",
      "Epoch 348/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0015\n",
      "Epoch 349/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0015\n",
      "Epoch 350/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0015\n",
      "Epoch 351/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014\n",
      "Epoch 352/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0014\n",
      "Epoch 353/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0014\n",
      "Epoch 354/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013\n",
      "Epoch 355/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013\n",
      "Epoch 356/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013\n",
      "Epoch 357/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0013\n",
      "Epoch 358/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012\n",
      "Epoch 359/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012\n",
      "Epoch 360/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012\n",
      "Epoch 361/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0012\n",
      "Epoch 362/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0011\n",
      "Epoch 363/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011\n",
      "Epoch 364/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011\n",
      "Epoch 365/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0011\n",
      "Epoch 366/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0010\n",
      "Epoch 367/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0010\n",
      "Epoch 368/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.9901e-04\n",
      "Epoch 369/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.7849e-04\n",
      "Epoch 370/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.5840e-04\n",
      "Epoch 371/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3871e-04\n",
      "Epoch 372/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.1943e-04\n",
      "Epoch 373/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.0054e-04\n",
      "Epoch 374/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.8204e-04\n",
      "Epoch 375/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.6393e-04\n",
      "Epoch 376/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 8.4618e-04\n",
      "Epoch 377/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 8.2880e-04\n",
      "Epoch 378/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.1178e-04\n",
      "Epoch 379/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.9510e-04\n",
      "Epoch 380/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7877e-04\n",
      "Epoch 381/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6277e-04\n",
      "Epoch 382/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.4710e-04\n",
      "Epoch 383/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.3176e-04\n",
      "Epoch 384/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.1673e-04\n",
      "Epoch 385/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.0201e-04\n",
      "Epoch 386/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.8759e-04\n",
      "Epoch 387/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.7347e-04\n",
      "Epoch 388/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.5964e-04\n",
      "Epoch 389/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.4609e-04\n",
      "Epoch 390/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.3281e-04\n",
      "Epoch 391/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.1981e-04\n",
      "Epoch 392/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.0708e-04\n",
      "Epoch 393/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.9461e-04\n",
      "Epoch 394/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.8240e-04\n",
      "Epoch 395/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.7043e-04\n",
      "Epoch 396/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.5872e-04\n",
      "Epoch 397/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.4724e-04\n",
      "Epoch 398/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.3600e-04\n",
      "Epoch 399/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.2499e-04\n",
      "Epoch 400/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 5.1421e-04\n",
      "Epoch 401/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.0364e-04\n",
      "Epoch 402/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.9330e-04\n",
      "Epoch 403/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 4.8316e-04\n",
      "Epoch 404/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.7324e-04\n",
      "Epoch 405/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.6352e-04\n",
      "Epoch 406/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.5400e-04\n",
      "Epoch 407/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.4467e-04\n",
      "Epoch 408/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.3554e-04\n",
      "Epoch 409/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.2659e-04\n",
      "Epoch 410/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.1783e-04\n",
      "Epoch 411/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.0925e-04\n",
      "Epoch 412/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 4.0084e-04\n",
      "Epoch 413/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.9261e-04\n",
      "Epoch 414/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.8454e-04\n",
      "Epoch 415/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.7664e-04\n",
      "Epoch 416/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.6891e-04\n",
      "Epoch 417/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.6133e-04\n",
      "Epoch 418/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.5391e-04\n",
      "Epoch 419/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.4664e-04\n",
      "Epoch 420/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.3952e-04\n",
      "Epoch 421/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.3254e-04\n",
      "Epoch 422/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.2571e-04\n",
      "Epoch 423/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.1902e-04\n",
      "Epoch 424/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.1247e-04\n",
      "Epoch 425/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.0605e-04\n",
      "Epoch 426/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.9976e-04\n",
      "Epoch 427/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.9361e-04\n",
      "Epoch 428/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.8757e-04\n",
      "Epoch 429/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.8167e-04\n",
      "Epoch 430/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.7588e-04\n",
      "Epoch 431/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.7021e-04\n",
      "Epoch 432/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.6466e-04\n",
      "Epoch 433/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.5923e-04\n",
      "Epoch 434/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.5390e-04\n",
      "Epoch 435/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.4869e-04\n",
      "Epoch 436/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.4358e-04\n",
      "Epoch 437/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.3858e-04\n",
      "Epoch 438/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3368e-04\n",
      "Epoch 439/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.2887e-04\n",
      "Epoch 440/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.2417e-04\n",
      "Epoch 441/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.1957e-04\n",
      "Epoch 442/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.1506e-04\n",
      "Epoch 443/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.1064e-04\n",
      "Epoch 444/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.0631e-04\n",
      "Epoch 445/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.0208e-04\n",
      "Epoch 446/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.9792e-04\n",
      "Epoch 447/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.9386e-04\n",
      "Epoch 448/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.8988e-04\n",
      "Epoch 449/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.8598e-04\n",
      "Epoch 450/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.8215e-04\n",
      "Epoch 451/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7841e-04\n",
      "Epoch 452/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7475e-04\n",
      "Epoch 453/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.7116e-04\n",
      "Epoch 454/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.6764e-04\n",
      "Epoch 455/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.6420e-04\n",
      "Epoch 456/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.6083e-04\n",
      "Epoch 457/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.5752e-04\n",
      "Epoch 458/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5429e-04\n",
      "Epoch 459/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5112e-04\n",
      "Epoch 460/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4801e-04\n",
      "Epoch 461/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4497e-04\n",
      "Epoch 462/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4199e-04\n",
      "Epoch 463/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 1.3908e-04\n",
      "Epoch 464/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3622e-04\n",
      "Epoch 465/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3342e-04\n",
      "Epoch 466/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.3068e-04\n",
      "Epoch 467/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.2800e-04\n",
      "Epoch 468/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2537e-04\n",
      "Epoch 469/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2279e-04\n",
      "Epoch 470/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.2027e-04\n",
      "Epoch 471/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.1780e-04\n",
      "Epoch 472/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1538e-04\n",
      "Epoch 473/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.1301e-04\n",
      "Epoch 474/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.1069e-04\n",
      "Epoch 475/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0842e-04\n",
      "Epoch 476/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.0619e-04\n",
      "Epoch 477/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.0401e-04\n",
      "Epoch 478/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0187e-04\n",
      "Epoch 479/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.9780e-05\n",
      "Epoch 480/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.7731e-05\n",
      "Epoch 481/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.5723e-05\n",
      "Epoch 482/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.3757e-05\n",
      "Epoch 483/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.1830e-05\n",
      "Epoch 484/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.9944e-05\n",
      "Epoch 485/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.8096e-05\n",
      "Epoch 486/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 8.6288e-05\n",
      "Epoch 487/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 8.4515e-05\n",
      "Epoch 488/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.2780e-05\n",
      "Epoch 489/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 8.1079e-05\n",
      "Epoch 490/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.9413e-05\n",
      "Epoch 491/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.7781e-05\n",
      "Epoch 492/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6185e-05\n",
      "Epoch 493/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 7.4619e-05\n",
      "Epoch 494/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.3087e-05\n",
      "Epoch 495/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 7.1586e-05\n",
      "Epoch 496/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.0116e-05\n",
      "Epoch 497/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.8675e-05\n",
      "Epoch 498/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.7265e-05\n",
      "Epoch 499/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 6.5883e-05\n",
      "Epoch 500/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.4530e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x177949220>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fit the model and train for 500 epochs\n",
    "model.fit(xs, ys, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "6LTIB5GgRuX7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "[[18.976562]]\n"
     ]
    }
   ],
   "source": [
    "# predict with value of 10\n",
    "tf_pred = model.predict(np.array([10.0]))\n",
    "print(tf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Xg-EhY6TkVa"
   },
   "source": [
    "# II.2 Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "Ad15862VTn38"
   },
   "outputs": [],
   "source": [
    "# for pytorch we need to convert the list to tensor\n",
    "xs = [[-1.0],  [0.0], [1.0], [2.0], [3.0], [4.0]]\n",
    "ys = [[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]]\n",
    "xs = Variable(torch.Tensor(xs))\n",
    "ys = Variable(torch.Tensor(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "7B31YB7rUD6p"
   },
   "outputs": [],
   "source": [
    "# we define RegressionModel class with one linear layer and instanciate the model\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "mrlpsM4YUii3"
   },
   "outputs": [],
   "source": [
    "# Choose an appropriate loss function for regression (e.g., MSELoss)\n",
    "# and an optimizer (e.g., SGD) with a learning rate of 0.001.\n",
    "##<YOUR CODE>\n",
    "criterion = torch.nn.MSELoss() # TODO: Your code to define the loss function\n",
    "optimizer = torch.optim.SGD(lr=0.001, params=model.parameters()) # TODO: Your code to define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "-vj_erFoUvC7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 35.96160888671875\n",
      "epoch 1, loss 35.18720245361328\n",
      "epoch 2, loss 34.43013381958008\n",
      "epoch 3, loss 33.69001007080078\n",
      "epoch 4, loss 32.966453552246094\n",
      "epoch 5, loss 32.25908279418945\n",
      "epoch 6, loss 31.56754493713379\n",
      "epoch 7, loss 30.891477584838867\n",
      "epoch 8, loss 30.23052978515625\n",
      "epoch 9, loss 29.584379196166992\n",
      "epoch 10, loss 28.952674865722656\n",
      "epoch 11, loss 28.335105895996094\n",
      "epoch 12, loss 27.731340408325195\n",
      "epoch 13, loss 27.141077041625977\n",
      "epoch 14, loss 26.564016342163086\n",
      "epoch 15, loss 25.999849319458008\n",
      "epoch 16, loss 25.44829750061035\n",
      "epoch 17, loss 24.909072875976562\n",
      "epoch 18, loss 24.381898880004883\n",
      "epoch 19, loss 23.866506576538086\n",
      "epoch 20, loss 23.36263084411621\n",
      "epoch 21, loss 22.870012283325195\n",
      "epoch 22, loss 22.388395309448242\n",
      "epoch 23, loss 21.91753578186035\n",
      "epoch 24, loss 21.457197189331055\n",
      "epoch 25, loss 21.007129669189453\n",
      "epoch 26, loss 20.56711769104004\n",
      "epoch 27, loss 20.136926651000977\n",
      "epoch 28, loss 19.71633529663086\n",
      "epoch 29, loss 19.305130004882812\n",
      "epoch 30, loss 18.90310287475586\n",
      "epoch 31, loss 18.510046005249023\n",
      "epoch 32, loss 18.12575340270996\n",
      "epoch 33, loss 17.750038146972656\n",
      "epoch 34, loss 17.3826961517334\n",
      "epoch 35, loss 17.02354621887207\n",
      "epoch 36, loss 16.67240333557129\n",
      "epoch 37, loss 16.32908821105957\n",
      "epoch 38, loss 15.993422508239746\n",
      "epoch 39, loss 15.665236473083496\n",
      "epoch 40, loss 15.344359397888184\n",
      "epoch 41, loss 15.030632972717285\n",
      "epoch 42, loss 14.723892211914062\n",
      "epoch 43, loss 14.423980712890625\n",
      "epoch 44, loss 14.130744934082031\n",
      "epoch 45, loss 13.844036102294922\n",
      "epoch 46, loss 13.563708305358887\n",
      "epoch 47, loss 13.2896146774292\n",
      "epoch 48, loss 13.021617889404297\n",
      "epoch 49, loss 12.75958251953125\n",
      "epoch 50, loss 12.503373146057129\n",
      "epoch 51, loss 12.252856254577637\n",
      "epoch 52, loss 12.007908821105957\n",
      "epoch 53, loss 11.768403053283691\n",
      "epoch 54, loss 11.534217834472656\n",
      "epoch 55, loss 11.305233001708984\n",
      "epoch 56, loss 11.081332206726074\n",
      "epoch 57, loss 10.862401008605957\n",
      "epoch 58, loss 10.648327827453613\n",
      "epoch 59, loss 10.439003944396973\n",
      "epoch 60, loss 10.234322547912598\n",
      "epoch 61, loss 10.034177780151367\n",
      "epoch 62, loss 9.838471412658691\n",
      "epoch 63, loss 9.647101402282715\n",
      "epoch 64, loss 9.45997142791748\n",
      "epoch 65, loss 9.276985168457031\n",
      "epoch 66, loss 9.098050117492676\n",
      "epoch 67, loss 8.923075675964355\n",
      "epoch 68, loss 8.751973152160645\n",
      "epoch 69, loss 8.584653854370117\n",
      "epoch 70, loss 8.421034812927246\n",
      "epoch 71, loss 8.261033058166504\n",
      "epoch 72, loss 8.104568481445312\n",
      "epoch 73, loss 7.951557159423828\n",
      "epoch 74, loss 7.801926136016846\n",
      "epoch 75, loss 7.65559720993042\n",
      "epoch 76, loss 7.51249885559082\n",
      "epoch 77, loss 7.372556209564209\n",
      "epoch 78, loss 7.235697269439697\n",
      "epoch 79, loss 7.1018548011779785\n",
      "epoch 80, loss 6.970962047576904\n",
      "epoch 81, loss 6.842950820922852\n",
      "epoch 82, loss 6.717758655548096\n",
      "epoch 83, loss 6.595320224761963\n",
      "epoch 84, loss 6.475574493408203\n",
      "epoch 85, loss 6.358461380004883\n",
      "epoch 86, loss 6.243921756744385\n",
      "epoch 87, loss 6.131899356842041\n",
      "epoch 88, loss 6.022334575653076\n",
      "epoch 89, loss 5.9151740074157715\n",
      "epoch 90, loss 5.810363292694092\n",
      "epoch 91, loss 5.707849979400635\n",
      "epoch 92, loss 5.607583522796631\n",
      "epoch 93, loss 5.509512424468994\n",
      "epoch 94, loss 5.41358757019043\n",
      "epoch 95, loss 5.319760799407959\n",
      "epoch 96, loss 5.227985382080078\n",
      "epoch 97, loss 5.1382155418396\n",
      "epoch 98, loss 5.050405025482178\n",
      "epoch 99, loss 4.964511871337891\n",
      "epoch 100, loss 4.880491733551025\n",
      "epoch 101, loss 4.79830265045166\n",
      "epoch 102, loss 4.717904567718506\n",
      "epoch 103, loss 4.639256954193115\n",
      "epoch 104, loss 4.562319755554199\n",
      "epoch 105, loss 4.48705530166626\n",
      "epoch 106, loss 4.413426876068115\n",
      "epoch 107, loss 4.341396331787109\n",
      "epoch 108, loss 4.270927906036377\n",
      "epoch 109, loss 4.201988697052002\n",
      "epoch 110, loss 4.134542942047119\n",
      "epoch 111, loss 4.068556308746338\n",
      "epoch 112, loss 4.003996849060059\n",
      "epoch 113, loss 3.940833806991577\n",
      "epoch 114, loss 3.879035234451294\n",
      "epoch 115, loss 3.8185694217681885\n",
      "epoch 116, loss 3.7594077587127686\n",
      "epoch 117, loss 3.7015209197998047\n",
      "epoch 118, loss 3.6448795795440674\n",
      "epoch 119, loss 3.589456796646118\n",
      "epoch 120, loss 3.535224199295044\n",
      "epoch 121, loss 3.4821577072143555\n",
      "epoch 122, loss 3.430227279663086\n",
      "epoch 123, loss 3.379410982131958\n",
      "epoch 124, loss 3.3296823501586914\n",
      "epoch 125, loss 3.2810165882110596\n",
      "epoch 126, loss 3.233391046524048\n",
      "epoch 127, loss 3.186781883239746\n",
      "epoch 128, loss 3.1411664485931396\n",
      "epoch 129, loss 3.096522331237793\n",
      "epoch 130, loss 3.052827835083008\n",
      "epoch 131, loss 3.0100622177124023\n",
      "epoch 132, loss 2.9682044982910156\n",
      "epoch 133, loss 2.9272336959838867\n",
      "epoch 134, loss 2.8871309757232666\n",
      "epoch 135, loss 2.84787654876709\n",
      "epoch 136, loss 2.8094513416290283\n",
      "epoch 137, loss 2.7718379497528076\n",
      "epoch 138, loss 2.735016107559204\n",
      "epoch 139, loss 2.6989688873291016\n",
      "epoch 140, loss 2.663679838180542\n",
      "epoch 141, loss 2.62913179397583\n",
      "epoch 142, loss 2.5953078269958496\n",
      "epoch 143, loss 2.5621917247772217\n",
      "epoch 144, loss 2.5297679901123047\n",
      "epoch 145, loss 2.4980199337005615\n",
      "epoch 146, loss 2.466935157775879\n",
      "epoch 147, loss 2.4364967346191406\n",
      "epoch 148, loss 2.406691074371338\n",
      "epoch 149, loss 2.377502918243408\n",
      "epoch 150, loss 2.3489198684692383\n",
      "epoch 151, loss 2.3209283351898193\n",
      "epoch 152, loss 2.2935140132904053\n",
      "epoch 153, loss 2.26666522026062\n",
      "epoch 154, loss 2.2403690814971924\n",
      "epoch 155, loss 2.2146120071411133\n",
      "epoch 156, loss 2.1893842220306396\n",
      "epoch 157, loss 2.164672613143921\n",
      "epoch 158, loss 2.140465021133423\n",
      "epoch 159, loss 2.1167514324188232\n",
      "epoch 160, loss 2.0935206413269043\n",
      "epoch 161, loss 2.0707614421844482\n",
      "epoch 162, loss 2.048464059829712\n",
      "epoch 163, loss 2.0266172885894775\n",
      "epoch 164, loss 2.00521183013916\n",
      "epoch 165, loss 1.984237790107727\n",
      "epoch 166, loss 1.9636850357055664\n",
      "epoch 167, loss 1.9435447454452515\n",
      "epoch 168, loss 1.9238072633743286\n",
      "epoch 169, loss 1.9044647216796875\n",
      "epoch 170, loss 1.8855069875717163\n",
      "epoch 171, loss 1.8669260740280151\n",
      "epoch 172, loss 1.8487133979797363\n",
      "epoch 173, loss 1.8308619260787964\n",
      "epoch 174, loss 1.8133621215820312\n",
      "epoch 175, loss 1.796207070350647\n",
      "epoch 176, loss 1.7793887853622437\n",
      "epoch 177, loss 1.762900471687317\n",
      "epoch 178, loss 1.746733546257019\n",
      "epoch 179, loss 1.730881690979004\n",
      "epoch 180, loss 1.715338110923767\n",
      "epoch 181, loss 1.7000951766967773\n",
      "epoch 182, loss 1.6851468086242676\n",
      "epoch 183, loss 1.670486569404602\n",
      "epoch 184, loss 1.6561075448989868\n",
      "epoch 185, loss 1.6420040130615234\n",
      "epoch 186, loss 1.6281698942184448\n",
      "epoch 187, loss 1.614599347114563\n",
      "epoch 188, loss 1.6012858152389526\n",
      "epoch 189, loss 1.588223934173584\n",
      "epoch 190, loss 1.5754090547561646\n",
      "epoch 191, loss 1.562834620475769\n",
      "epoch 192, loss 1.5504956245422363\n",
      "epoch 193, loss 1.5383864641189575\n",
      "epoch 194, loss 1.526503086090088\n",
      "epoch 195, loss 1.5148391723632812\n",
      "epoch 196, loss 1.5033916234970093\n",
      "epoch 197, loss 1.4921542406082153\n",
      "epoch 198, loss 1.4811229705810547\n",
      "epoch 199, loss 1.470293402671814\n",
      "epoch 200, loss 1.4596604108810425\n",
      "epoch 201, loss 1.4492205381393433\n",
      "epoch 202, loss 1.4389691352844238\n",
      "epoch 203, loss 1.42890202999115\n",
      "epoch 204, loss 1.4190150499343872\n",
      "epoch 205, loss 1.4093049764633179\n",
      "epoch 206, loss 1.3997668027877808\n",
      "epoch 207, loss 1.3903971910476685\n",
      "epoch 208, loss 1.381192684173584\n",
      "epoch 209, loss 1.3721498250961304\n",
      "epoch 210, loss 1.3632644414901733\n",
      "epoch 211, loss 1.3545335531234741\n",
      "epoch 212, loss 1.3459539413452148\n",
      "epoch 213, loss 1.3375216722488403\n",
      "epoch 214, loss 1.3292344808578491\n",
      "epoch 215, loss 1.321088194847107\n",
      "epoch 216, loss 1.313079833984375\n",
      "epoch 217, loss 1.305207371711731\n",
      "epoch 218, loss 1.2974663972854614\n",
      "epoch 219, loss 1.2898552417755127\n",
      "epoch 220, loss 1.2823704481124878\n",
      "epoch 221, loss 1.2750099897384644\n",
      "epoch 222, loss 1.267770528793335\n",
      "epoch 223, loss 1.2606490850448608\n",
      "epoch 224, loss 1.2536438703536987\n",
      "epoch 225, loss 1.246752381324768\n",
      "epoch 226, loss 1.2399715185165405\n",
      "epoch 227, loss 1.2332996129989624\n",
      "epoch 228, loss 1.2267335653305054\n",
      "epoch 229, loss 1.220271348953247\n",
      "epoch 230, loss 1.2139109373092651\n",
      "epoch 231, loss 1.207650065422058\n",
      "epoch 232, loss 1.2014862298965454\n",
      "epoch 233, loss 1.1954180002212524\n",
      "epoch 234, loss 1.1894429922103882\n",
      "epoch 235, loss 1.1835588216781616\n",
      "epoch 236, loss 1.1777640581130981\n",
      "epoch 237, loss 1.1720563173294067\n",
      "epoch 238, loss 1.1664345264434814\n",
      "epoch 239, loss 1.1608959436416626\n",
      "epoch 240, loss 1.1554392576217651\n",
      "epoch 241, loss 1.1500626802444458\n",
      "epoch 242, loss 1.1447643041610718\n",
      "epoch 243, loss 1.1395426988601685\n",
      "epoch 244, loss 1.1343960762023926\n",
      "epoch 245, loss 1.12932288646698\n",
      "epoch 246, loss 1.1243218183517456\n",
      "epoch 247, loss 1.1193907260894775\n",
      "epoch 248, loss 1.1145285367965698\n",
      "epoch 249, loss 1.1097339391708374\n",
      "epoch 250, loss 1.105005145072937\n",
      "epoch 251, loss 1.1003409624099731\n",
      "epoch 252, loss 1.0957403182983398\n",
      "epoch 253, loss 1.0912011861801147\n",
      "epoch 254, loss 1.086722731590271\n",
      "epoch 255, loss 1.0823034048080444\n",
      "epoch 256, loss 1.0779422521591187\n",
      "epoch 257, loss 1.0736377239227295\n",
      "epoch 258, loss 1.0693891048431396\n",
      "epoch 259, loss 1.0651944875717163\n",
      "epoch 260, loss 1.0610538721084595\n",
      "epoch 261, loss 1.0569649934768677\n",
      "epoch 262, loss 1.0529271364212036\n",
      "epoch 263, loss 1.0489397048950195\n",
      "epoch 264, loss 1.0450010299682617\n",
      "epoch 265, loss 1.0411103963851929\n",
      "epoch 266, loss 1.0372666120529175\n",
      "epoch 267, loss 1.033469319343567\n",
      "epoch 268, loss 1.0297168493270874\n",
      "epoch 269, loss 1.0260084867477417\n",
      "epoch 270, loss 1.022343397140503\n",
      "epoch 271, loss 1.0187205076217651\n",
      "epoch 272, loss 1.0151394605636597\n",
      "epoch 273, loss 1.0115987062454224\n",
      "epoch 274, loss 1.0080980062484741\n",
      "epoch 275, loss 1.0046359300613403\n",
      "epoch 276, loss 1.001212477684021\n",
      "epoch 277, loss 0.9978262782096863\n",
      "epoch 278, loss 0.9944766163825989\n",
      "epoch 279, loss 0.9911630153656006\n",
      "epoch 280, loss 0.9878843426704407\n",
      "epoch 281, loss 0.9846405386924744\n",
      "epoch 282, loss 0.9814300537109375\n",
      "epoch 283, loss 0.9782530665397644\n",
      "epoch 284, loss 0.9751083850860596\n",
      "epoch 285, loss 0.9719955921173096\n",
      "epoch 286, loss 0.9689137935638428\n",
      "epoch 287, loss 0.9658623337745667\n",
      "epoch 288, loss 0.9628414511680603\n",
      "epoch 289, loss 0.9598493576049805\n",
      "epoch 290, loss 0.9568861126899719\n",
      "epoch 291, loss 0.9539510607719421\n",
      "epoch 292, loss 0.9510440230369568\n",
      "epoch 293, loss 0.9481637477874756\n",
      "epoch 294, loss 0.9453100562095642\n",
      "epoch 295, loss 0.9424822926521301\n",
      "epoch 296, loss 0.9396803379058838\n",
      "epoch 297, loss 0.936903178691864\n",
      "epoch 298, loss 0.9341508746147156\n",
      "epoch 299, loss 0.9314222931861877\n",
      "epoch 300, loss 0.9287176132202148\n",
      "epoch 301, loss 0.9260361194610596\n",
      "epoch 302, loss 0.9233770966529846\n",
      "epoch 303, loss 0.9207406640052795\n",
      "epoch 304, loss 0.9181260466575623\n",
      "epoch 305, loss 0.915532648563385\n",
      "epoch 306, loss 0.9129607677459717\n",
      "epoch 307, loss 0.910409152507782\n",
      "epoch 308, loss 0.9078781604766846\n",
      "epoch 309, loss 0.9053667187690735\n",
      "epoch 310, loss 0.9028746485710144\n",
      "epoch 311, loss 0.9004020690917969\n",
      "epoch 312, loss 0.8979482650756836\n",
      "epoch 313, loss 0.895512580871582\n",
      "epoch 314, loss 0.8930951952934265\n",
      "epoch 315, loss 0.8906957507133484\n",
      "epoch 316, loss 0.8883137106895447\n",
      "epoch 317, loss 0.885948657989502\n",
      "epoch 318, loss 0.8836005330085754\n",
      "epoch 319, loss 0.8812689781188965\n",
      "epoch 320, loss 0.8789535164833069\n",
      "epoch 321, loss 0.8766541481018066\n",
      "epoch 322, loss 0.8743705153465271\n",
      "epoch 323, loss 0.8721020817756653\n",
      "epoch 324, loss 0.8698486685752869\n",
      "epoch 325, loss 0.8676104545593262\n",
      "epoch 326, loss 0.8653864860534668\n",
      "epoch 327, loss 0.8631770014762878\n",
      "epoch 328, loss 0.8609814047813416\n",
      "epoch 329, loss 0.8587997555732727\n",
      "epoch 330, loss 0.8566319942474365\n",
      "epoch 331, loss 0.8544773459434509\n",
      "epoch 332, loss 0.8523358702659607\n",
      "epoch 333, loss 0.8502071499824524\n",
      "epoch 334, loss 0.8480914235115051\n",
      "epoch 335, loss 0.8459879755973816\n",
      "epoch 336, loss 0.8438968658447266\n",
      "epoch 337, loss 0.8418179154396057\n",
      "epoch 338, loss 0.8397505879402161\n",
      "epoch 339, loss 0.8376950621604919\n",
      "epoch 340, loss 0.8356512188911438\n",
      "epoch 341, loss 0.8336184620857239\n",
      "epoch 342, loss 0.831597089767456\n",
      "epoch 343, loss 0.8295864462852478\n",
      "epoch 344, loss 0.8275866508483887\n",
      "epoch 345, loss 0.8255972266197205\n",
      "epoch 346, loss 0.8236185908317566\n",
      "epoch 347, loss 0.8216497898101807\n",
      "epoch 348, loss 0.8196914196014404\n",
      "epoch 349, loss 0.8177428245544434\n",
      "epoch 350, loss 0.8158041834831238\n",
      "epoch 351, loss 0.8138751983642578\n",
      "epoch 352, loss 0.8119556307792664\n",
      "epoch 353, loss 0.8100452423095703\n",
      "epoch 354, loss 0.8081443309783936\n",
      "epoch 355, loss 0.8062524795532227\n",
      "epoch 356, loss 0.8043694496154785\n",
      "epoch 357, loss 0.8024953007698059\n",
      "epoch 358, loss 0.8006297945976257\n",
      "epoch 359, loss 0.7987729907035828\n",
      "epoch 360, loss 0.7969244122505188\n",
      "epoch 361, loss 0.7950842380523682\n",
      "epoch 362, loss 0.7932524085044861\n",
      "epoch 363, loss 0.7914283275604248\n",
      "epoch 364, loss 0.789612352848053\n",
      "epoch 365, loss 0.7878043055534363\n",
      "epoch 366, loss 0.7860040068626404\n",
      "epoch 367, loss 0.7842112183570862\n",
      "epoch 368, loss 0.7824260592460632\n",
      "epoch 369, loss 0.7806482911109924\n",
      "epoch 370, loss 0.7788779139518738\n",
      "epoch 371, loss 0.7771148681640625\n",
      "epoch 372, loss 0.7753588557243347\n",
      "epoch 373, loss 0.7736096978187561\n",
      "epoch 374, loss 0.7718676924705505\n",
      "epoch 375, loss 0.7701324820518494\n",
      "epoch 376, loss 0.7684041857719421\n",
      "epoch 377, loss 0.7666824460029602\n",
      "epoch 378, loss 0.7649674415588379\n",
      "epoch 379, loss 0.7632589340209961\n",
      "epoch 380, loss 0.7615568041801453\n",
      "epoch 381, loss 0.759861171245575\n",
      "epoch 382, loss 0.758171796798706\n",
      "epoch 383, loss 0.7564885020256042\n",
      "epoch 384, loss 0.7548112869262695\n",
      "epoch 385, loss 0.7531404495239258\n",
      "epoch 386, loss 0.7514753341674805\n",
      "epoch 387, loss 0.749816358089447\n",
      "epoch 388, loss 0.7481630444526672\n",
      "epoch 389, loss 0.7465155124664307\n",
      "epoch 390, loss 0.7448738217353821\n",
      "epoch 391, loss 0.7432376742362976\n",
      "epoch 392, loss 0.7416071891784668\n",
      "epoch 393, loss 0.7399823069572449\n",
      "epoch 394, loss 0.7383629679679871\n",
      "epoch 395, loss 0.7367488741874695\n",
      "epoch 396, loss 0.7351401448249817\n",
      "epoch 397, loss 0.7335367202758789\n",
      "epoch 398, loss 0.7319388389587402\n",
      "epoch 399, loss 0.7303459048271179\n",
      "epoch 400, loss 0.7287580370903015\n",
      "epoch 401, loss 0.727175235748291\n",
      "epoch 402, loss 0.725597620010376\n",
      "epoch 403, loss 0.7240250110626221\n",
      "epoch 404, loss 0.7224571704864502\n",
      "epoch 405, loss 0.7208942770957947\n",
      "epoch 406, loss 0.7193363308906555\n",
      "epoch 407, loss 0.7177830338478088\n",
      "epoch 408, loss 0.7162346243858337\n",
      "epoch 409, loss 0.7146907448768616\n",
      "epoch 410, loss 0.713151752948761\n",
      "epoch 411, loss 0.7116172313690186\n",
      "epoch 412, loss 0.7100874781608582\n",
      "epoch 413, loss 0.7085619568824768\n",
      "epoch 414, loss 0.7070410847663879\n",
      "epoch 415, loss 0.7055248618125916\n",
      "epoch 416, loss 0.7040126919746399\n",
      "epoch 417, loss 0.7025051712989807\n",
      "epoch 418, loss 0.7010018229484558\n",
      "epoch 419, loss 0.6995029449462891\n",
      "epoch 420, loss 0.6980082988739014\n",
      "epoch 421, loss 0.6965177059173584\n",
      "epoch 422, loss 0.6950315833091736\n",
      "epoch 423, loss 0.693549394607544\n",
      "epoch 424, loss 0.6920714974403381\n",
      "epoch 425, loss 0.6905975937843323\n",
      "epoch 426, loss 0.6891279816627502\n",
      "epoch 427, loss 0.6876620650291443\n",
      "epoch 428, loss 0.6862004399299622\n",
      "epoch 429, loss 0.6847425103187561\n",
      "epoch 430, loss 0.6832887530326843\n",
      "epoch 431, loss 0.6818388104438782\n",
      "epoch 432, loss 0.6803929209709167\n",
      "epoch 433, loss 0.6789507269859314\n",
      "epoch 434, loss 0.6775122284889221\n",
      "epoch 435, loss 0.6760777831077576\n",
      "epoch 436, loss 0.6746470332145691\n",
      "epoch 437, loss 0.6732200980186462\n",
      "epoch 438, loss 0.6717967391014099\n",
      "epoch 439, loss 0.6703771948814392\n",
      "epoch 440, loss 0.6689612865447998\n",
      "epoch 441, loss 0.6675490736961365\n",
      "epoch 442, loss 0.6661403775215149\n",
      "epoch 443, loss 0.6647353768348694\n",
      "epoch 444, loss 0.6633338928222656\n",
      "epoch 445, loss 0.6619359850883484\n",
      "epoch 446, loss 0.6605416536331177\n",
      "epoch 447, loss 0.6591507792472839\n",
      "epoch 448, loss 0.6577636003494263\n",
      "epoch 449, loss 0.656379759311676\n",
      "epoch 450, loss 0.6549991965293884\n",
      "epoch 451, loss 0.6536223292350769\n",
      "epoch 452, loss 0.652248740196228\n",
      "epoch 453, loss 0.6508784890174866\n",
      "epoch 454, loss 0.6495115756988525\n",
      "epoch 455, loss 0.648148238658905\n",
      "epoch 456, loss 0.6467880010604858\n",
      "epoch 457, loss 0.6454312801361084\n",
      "epoch 458, loss 0.6440777778625488\n",
      "epoch 459, loss 0.6427274942398071\n",
      "epoch 460, loss 0.6413804292678833\n",
      "epoch 461, loss 0.6400368213653564\n",
      "epoch 462, loss 0.6386963129043579\n",
      "epoch 463, loss 0.6373590230941772\n",
      "epoch 464, loss 0.6360248923301697\n",
      "epoch 465, loss 0.63469398021698\n",
      "epoch 466, loss 0.6333663463592529\n",
      "epoch 467, loss 0.6320417523384094\n",
      "epoch 468, loss 0.6307201981544495\n",
      "epoch 469, loss 0.6294017434120178\n",
      "epoch 470, loss 0.6280864477157593\n",
      "epoch 471, loss 0.6267743110656738\n",
      "epoch 472, loss 0.6254650950431824\n",
      "epoch 473, loss 0.624159038066864\n",
      "epoch 474, loss 0.622856080532074\n",
      "epoch 475, loss 0.6215561032295227\n",
      "epoch 476, loss 0.6202589273452759\n",
      "epoch 477, loss 0.6189650297164917\n",
      "epoch 478, loss 0.6176738739013672\n",
      "epoch 479, loss 0.6163859963417053\n",
      "epoch 480, loss 0.6151009202003479\n",
      "epoch 481, loss 0.6138187050819397\n",
      "epoch 482, loss 0.6125394701957703\n",
      "epoch 483, loss 0.6112632155418396\n",
      "epoch 484, loss 0.6099898815155029\n",
      "epoch 485, loss 0.6087193489074707\n",
      "epoch 486, loss 0.6074517369270325\n",
      "epoch 487, loss 0.6061870455741882\n",
      "epoch 488, loss 0.6049251556396484\n",
      "epoch 489, loss 0.6036661267280579\n",
      "epoch 490, loss 0.6024099588394165\n",
      "epoch 491, loss 0.6011566519737244\n",
      "epoch 492, loss 0.5999060273170471\n",
      "epoch 493, loss 0.5986583232879639\n",
      "epoch 494, loss 0.5974134802818298\n",
      "epoch 495, loss 0.5961713194847107\n",
      "epoch 496, loss 0.594931960105896\n",
      "epoch 497, loss 0.5936952233314514\n",
      "epoch 498, loss 0.5924614071846008\n",
      "epoch 499, loss 0.5912303328514099\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(500):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing\n",
    "    # x to the model\n",
    "    pred_y = model(xs)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(pred_y, ys)\n",
    "\n",
    "    # Zero gradients, perform a backward pass,\n",
    "    # and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "Ag4QEF-gU6_4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.6539363861084\n"
     ]
    }
   ],
   "source": [
    "new_var = Variable(torch.Tensor([[10.0]]))\n",
    "torch_pred = model(new_var)\n",
    "print(model(new_var).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlmKn7mgVANd"
   },
   "source": [
    "# **Questions:**\n",
    "\n",
    "Compare and contrast the performance of the TensorFlow and PyTorch models. Consider the following:\n",
    "\n",
    "- Accuracy: Which model achieved higher accuracy in this prediction? Quantify the difference if possible.\n",
    "\n",
    "- Complexity: Briefly discuss the relative complexity of implementing the models in each framework based on your experience. Do you think the difference in accuracy justifies any potential increase in complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Answer:**\n",
    "\n",
    "**Accuracy:** TensorFlow achieved higher accuracy. For x=10 (expected: 19.0 | y = 2x - 1):\n",
    "- TensorFlow: ~18.98 (error: 0.02, 99.9% accurate)\n",
    "- PyTorch: ~16.65 (error: 2.35, 87.6% accurate)\n",
    "\n",
    "**Complexity:** \n",
    "\n",
    "Both have similar implementation complexity, Tensorflow is more concise with higher level of abstraction, while PyTorch requires creating a custom class and training loop. The accuracy difference for this problem justifies the implementation complexity of using Tensorflow, this might change depending on the problem, if you need more control. The differences could also be due to hyperparameters and framework implementation rather than framework limtitations, so it would depend on the problem."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMdA6dWPS+NZrpBKSV48EZ4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lab3-deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
